= Building production-ready AI infrastructure with vLLM Playground

Welcome to the vLLM Playground demonstration.

== What this demo covers

This demonstration shows how ACME Corporation is building production-ready AI infrastructure with vLLM Playground:

* Deploy vLLM servers via container management using Podman for rapid model serving
* Use modern chat interfaces with streaming responses to deliver real-time customer experiences
* Implement structured outputs using JSON Schema, Regex, and Grammar for reliable system integration
* Configure tool calling with open source large language models (Qwen, Llama, Mistral) to extend AI capabilities
* Set up Model Context Protocol (MCP) servers for agentic workflows with human-in-the-loop approval
* Run performance benchmarks with GuideLLM to validate production readiness

== Who this demo is for

This demonstration is designed for:

* **AI Engineers and Architects** evaluating AI inference platforms
* **Platform Engineers** building enterprise AI infrastructure
* **Technical Decision Makers** assessing AI serving solutions
* **Developers** integrating LLMs into applications

**Experience level**: Technical audience with basic understanding of containers, Linux, and AI/ML concepts

== Demo formats

This demonstration is modular and can be tailored to time constraints:

* **15-20 minutes (Executive Brief)**: Core business value + vLLM deployment demonstration
* **30-45 minutes (Standard Technical Demo)**: Getting Started, Structured Outputs, and Tool Calling capabilities
* **60 minutes (Deep Technical Dive)**: Complete demonstration of all 5 advanced features with performance benchmarking

The presenter will adapt the content based on audience interests and available time.

== ACME Corporation scenario

Throughout this demo, we follow **ACME Corporation's journey** to modernize their customer support infrastructure:

* **Business Challenge**: Build AI-powered customer support that handles increasing demand while maintaining response quality
* **Technical Requirements**: Flexible AI inference platform serving multiple models with tool integration and enterprise scalability
* **Solution**: vLLM Playground / Red Hat AI Inference Server (RHAIIS) providing high-performance LLM inference with advanced features

== Demo environment

For this demonstration, the environment is pre-configured with:

* GPU-enabled inference host: {targethost}
* vLLM Playground v0.1.1 pre-installed
* SSH access available: `{ssh_command}`
* Web UI: link:{vllm_playground_url}[Open vLLM Playground^]

NOTE: All environment details and credentials are available for hands-on exploration after the demo.

== Estimated timing

* **Full demonstration**: 60 minutes (all 5 modules)
* **Standard demo**: 30-45 minutes (modules 1-3)
* **Executive brief**: 15-20 minutes (module 1 + highlights)

The presenter will customize the flow based on audience priorities.

== Get started

Click on **Demo Overview** in the navigation to understand ACME's business challenge and how vLLM Playground addresses their AI infrastructure requirements.
