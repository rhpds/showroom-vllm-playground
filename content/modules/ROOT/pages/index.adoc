= Building Production-Ready AI Infrastructure with vLLM Playground

Welcome to the vLLM Playground demonstration!

== What you'll see in this demo

In this demonstration, we'll show you how ACME Corporation is building production-ready AI infrastructure with vLLM Playground:

* Deploy vLLM servers via container management using Podman for rapid model serving
* Use modern chat interfaces with streaming responses to deliver real-time customer experiences
* Implement structured outputs using JSON Schema, Regex, and Grammar for reliable system integration
* Configure tool calling with Open Source Large Language Models (Qwen, Llama, Mistral) to extend AI capabilities
* Set up Model Context Protocol (MCP) servers for agentic workflows with human-in-the-loop approval
* Run performance benchmarks with GuideLLM to validate production readiness

== Who this demo is for

This demonstration is designed for:

* **AI Engineers and Architects** evaluating AI inference platforms
* **Platform Engineers** building enterprise AI infrastructure
* **Technical Decision Makers** assessing AI serving solutions
* **Developers** integrating LLMs into applications

**Experience level**: Technical audience with basic understanding of containers, Linux, and AI/ML concepts

== Demo formats

This demonstration is modular and can be tailored to your time constraints:

* **15-20 minutes (Executive Brief)**: Core business value + vLLM deployment demonstration
* **30-45 minutes (Standard Technical Demo)**: Getting Started, Structured Outputs, and Tool Calling capabilities
* **60 minutes (Deep Technical Dive)**: Complete demonstration of all 5 advanced features with performance benchmarking

Your presenter will adapt the content based on your interests and available time.

== ACME Corporation scenario

Throughout this demo, we'll follow **ACME Corporation's journey** to modernize their customer support infrastructure:

* **Business Challenge**: Build AI-powered customer support that handles increasing demand while maintaining response quality
* **Technical Requirements**: Flexible AI inference platform serving multiple models with tool integration and enterprise scalability
* **Solution**: vLLM Playground / Red Hat AI Inference Server (RHAIIS) providing high-performance LLM inference with advanced features

== Demo environment

For this demonstration, we're using a pre-configured environment with:

* GPU-enabled inference host: {targethost}
* vLLM Playground v0.1.1 pre-installed
* SSH access available: `{ssh_command}`
* Web UI accessible at: http://{targethost}:7860

NOTE: All environment details and credentials are available for hands-on exploration after the demo.

== Estimated timing

* **Full demonstration**: 60 minutes (all 5 modules)
* **Standard demo**: 30-45 minutes (modules 1-3)
* **Executive brief**: 15-20 minutes (module 1 + highlights)

Your presenter will customize the flow based on your priorities.

== Let's get started!

Click on **Demo Overview** in the navigation to understand ACME's business challenge and how vLLM Playground addresses their AI infrastructure requirements.
