= vLLM Playground Workshop

Welcome to the vLLM Playground workshop!

== What you'll learn

In this workshop, you will:

* Deploy vLLM servers via container management using Podman
* Use the modern chat UI with streaming responses to interact with LLMs
* Implement structured outputs using JSON Schema, Regex, and Grammar
* Configure and use tool calling with various Open Source Large Language models (Qwen, Llama, Mistral, etc)
* Set up MCP (Model Context Protocol) servers for agentic capabilities
* Run performance benchmarks with GuideLLM

== Who this is for

This workshop is designed for Developers, AI Engineers, Platform Engineers, and Architects who want to set up a complete AI inference environment with tool calling and MCP integration.

**Experience level**: All levels - Beginner, Intermediate, and Advanced

== Prerequisites

Before starting this workshop, you should have:

* Basic knowledge of Red Hat Enterprise Linux (RHEL)
* Basic knowledge of vLLM (https://github.com/vllm-project/vllm), a high-throughput and memory-efficient inference and serving engine for LLMs.
* Familiarity with containers and container runtimes
* Basic understanding of AI inferencing concepts

== Workshop environment

Your lab environment comes pre-configured with vLLM Playground v0.1.1 and all necessary dependencies installed on a GPU-enabled machine.

You will have access to:

* Lab host: {targethost}
* SSH access: `{ssh_command}`
* Username: {ssh_username}
* Password: {ssh_password}

NOTE: All environment details and credentials are available in the lab interface.

== Estimated time

This workshop will take approximately *90 minutes* to complete.

You can work at your own pace.

== Let's get started!

Click on *Workshop Overview* in the navigation to begin your learning journey.
