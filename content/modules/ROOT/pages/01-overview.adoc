= vLLM Playground Demo Overview
:toc:
:toc-placement: preamble
:icons: font

== Customer scenario: ACME Corporation

ACME Corporation is evaluating AI inference solutions to modernize their customer support infrastructure. A senior AI engineer at ACME has been tasked with assessing vLLM / Red Hat AI Inference Server (RHAIIS) as the foundation for their AI inference service.

ACME needs to transform their customer support to handle increasing demand while maintaining response quality. Their VP of Engineering has explained: "We need a flexible AI inference platform that can serve multiple models, integrate with external tools, and scale to meet our enterprise requirements."

**The evaluation**: Assess vLLM / Red Hat AI Inference Server (RHAIIS) as the foundation for ACME's AI inference service, demonstrating capabilities for model serving, tool calling, and agentic workflows.

**Success criteria**: Prove that vLLM / RHAIIS can deliver high-performance LLM inference with advanced features like structured outputs, tool calling, and Model Context Protocol (MCP) integration.

== Business outcomes this demo delivers

This demonstration shows how ACME addresses their customer support challenges through practical AI inference capabilities:

* **Deploy and manage vLLM servers using Podman containers** - Establish reliable model serving infrastructure that eliminates manual deployment overhead
* **Interact with LLMs through modern chat interfaces** - Demonstrate real-time customer interaction capabilities with streaming responses
* **Configure structured outputs for consistent AI responses** - Ensure predictable, parseable data for downstream system integration
* **Implement tool calling for dynamic functionality** - Enable AI to execute actions and retrieve information from external systems
* **Set up MCP servers for agentic capabilities** - Extend AI with external tool access and human-in-the-loop approval workflows
* **Run performance benchmarks to validate throughput** - Prove the system can handle production workloads with quantified metrics

**Technical outcome**: Demonstrated experience with vLLM / RHAIIS that directly applies to enterprise AI infrastructure requirements.

**Business benefit**: A proven AI inference platform that enables intelligent customer support with enterprise-grade reliability and scalability.

== Target audience

This demonstration addresses the needs of:

* **AI Engineers** building inference infrastructure and evaluating serving solutions
* **Developers** integrating LLMs into applications and needing reliable inference APIs
* **Platform Engineers** evaluating AI serving solutions for enterprise deployment
* **Architects** designing enterprise AI systems and technology stacks

== Presenter preparation

To deliver this demonstration effectively, presenters should have:

* Basic Red Hat Enterprise Linux (RHEL) experience, including familiarity with Linux terminal operations
* Understanding of containers and Podman, including how container-based deployments work
* Basic AI/ML concepts, including understanding of what LLMs and inference mean
* Familiarity with customer support use cases for AI deployment

== ACME Corporation's AI infrastructure challenges

**The business situation**: ACME Corporation needs to build an AI-powered customer support system that handles diverse customer inquiries efficiently while scaling to enterprise demand.

**Project objective**: Evaluate vLLM / RHAIIS and demonstrate its capabilities for ACME's enterprise AI requirements with advanced features like tool calling and agentic workflows.

**Current operational challenges**:

* **Inconsistent AI responses**: Different models produce varying output formats, which complicates integration with existing CRM and ticketing systems
* **Limited tool integration**: Current AI solutions cannot execute actions or access external data, which reduces automation potential and requires manual intervention
* **Manual model management**: Deploying and updating models requires significant manual effort, which slows down iteration cycles and delays feature deployment
* **Scalability uncertainty**: Unclear if current approach can handle production traffic, which risks poor customer experience during peak periods

**The opportunity**: vLLM Playground provides a unified platform for model serving with advanced features that address these challenges. ACME's AI team has been selected to evaluate its feasibility for their customer support use case.

**Technical perspective**: "vLLM is the industry-leading high-performance inference engine with proven scalability. vLLM Playground provides the management interface we need to operationalize it. We need to evaluate how it handles our specific enterprise requirements for structured outputs, tool integration, and production performance."

== Transformation vision: ACME's success with vLLM

If vLLM Playground proves effective for ACME's use case, here are the anticipated improvements:

**Immediate improvements** (short-term wins):

* **Faster model deployment**: Deploy new models in minutes via containers, accelerating experimentation and iteration from days to minutes
* **Consistent AI outputs**: Structured outputs ensure predictable response formats, simplifying integration with downstream CRM and ticketing systems
* **Reduced integration complexity**: Standard APIs and consistent formats eliminate custom parsing logic and reduce maintenance overhead

**Strategic benefits** (long-term value):

* **Agentic capabilities**: MCP integration enables AI to use external tools, expanding automation possibilities for ticket routing, knowledge base search, and customer data retrieval
* **Human-in-the-loop approval**: Safe tool execution with manual approval gates, maintaining control over AI actions while enabling advanced workflows
* **Performance visibility**: Built-in benchmarking validates throughput and latency, enabling confident capacity planning and SLA commitments
* **Enterprise scalability**: Proven high-performance inference engine, supporting growing customer support volume without degradation

**Success metric**: A demonstrable AI inference platform that serves multiple models with tool calling, structured outputs, and agentic capabilities suitable for ACME's customer support requirements at enterprise scale.

== Common questions presenters should address

**"Can we use different models for different use cases?"**
-> Yes. vLLM Playground supports various models including Llama, Mistral, Qwen, and others with model-specific optimizations. You can deploy multiple vLLM servers simultaneously for different purposes.

**"How do we control AI outputs for integration with our existing systems?"**
-> Module 2 demonstrates structured outputs using JSON Schema, Regex, and Grammar to constrain responses. This ensures consistent formats for downstream system integration.

**"How does tool calling work with our customer support scenarios?"**
-> Module 3 shows defining custom tools that AI can invoke to retrieve customer data from CRMs, search knowledge bases, or execute support actions like ticket creation and routing.

**"Can we extend AI with access to our internal systems safely?"**
-> Module 4 demonstrates MCP integration for connecting AI to external tools with human-in-the-loop approval. This enables safe access to internal systems while maintaining control.

**"How do we validate the system can handle production workloads?"**
-> Module 5 covers performance benchmarking with GuideLLM to measure throughput, latency, and resource utilization. This provides quantified data for capacity planning and SLA commitments.

**"What about ongoing operational costs and resource requirements?"**
-> vLLM's memory-efficient architecture and GPU optimization reduce infrastructure costs. The demo shows resource monitoring and optimization techniques for cost-effective operation.

== Demonstration flow and timing guidance

=== 15-20 minute executive brief (Module 1 only)

* **Focus**: Core business value and vLLM deployment demonstration
* **Highlights**: Show rapid model serving with Podman, chat interface demo
* **Skip**: Detailed technical features, save for longer demos

=== 30-45 minute standard technical demo (Modules 1-3)

* **Focus**: Getting Started, Structured Outputs, Tool Calling
* **Highlights**: Model deployment, JSON Schema validation, custom tool definition
* **Skip or summarize**: MCP integration and performance testing

=== 60 minute deep technical dive (All 5 modules)

* **Focus**: Complete demonstration of all capabilities
* **Highlights**: Full technical depth with performance benchmarking
* **Include**: All advanced features, real-world integration scenarios

NOTE: Adapt the demonstration based on audience priorities and time constraints. Each module is self-contained and can be demonstrated independently or combined based on customer interests.
