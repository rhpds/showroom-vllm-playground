= Module 3: Advanced Inferencing: Tool Calling
:source-highlighter: rouge
:toc: macro
:toclevels: 1

In Module 2, structured outputs constrained AI responses to specific formats. Now ACME Corporation wants to take their AI customer support to the next level: enabling the AI to not just respond, but to take actions, such as looking up order status, checking inventory, or scheduling callbacks. This capability reduces average customer support handling time from 8 minutes to 3 minutes (62% faster), enabling ACME to handle 2.5x more customer inquiries with the same support team while improving first-contact resolution rates from 65% to 85%.

Tool calling (also known as function calling) allows the AI to recognize when it needs external data or actions, and generate structured function calls that systems can execute. This module demonstrates how to configure tool calling and define custom tools for ACME's customer support scenarios. By implementing tool calling, ACME achieves $400,000 annual savings through improved agent productivity and reduced escalation rates.

NOTE: This module focuses on the tool calling *pattern*, how the AI generates function calls. Actual tool execution with human-in-the-loop approval is covered in Module 4 (MCP Integration).

[[part-1]]
== Part 1 - Enabling Tool Calling

=== Know

_Customer challenge: ACME's engineering team needs to move AI from passive responses to active assistance. Today the AI can only answer questions. It cannot look up data, trigger actions, or integrate with backend systems, forcing support agents to manually handle every request._

**Business Impact:**

* Support agents spend 62% of their time on manual data lookups that AI could automate
* Customers must repeat information across multiple channels because AI cannot access backend systems
* First-contact resolution sits at 65% because AI cannot take action on customer requests
* ACME loses $400,000 annually in unnecessary escalations and agent overhead

**Value Proposition:**

Tool calling enables the AI to recognize when it needs external data, then generate structured function calls that backend systems can execute. Combined with proper tool definitions, this transforms AI from a conversational responder into an actionable assistant that integrates with ACME's existing order management, CRM, and scheduling systems.

=== Show

**What I say:**

"Now that we have structured outputs working, let me show you how tool calling takes things further. Instead of just formatting responses, the AI can now recognize when it needs real data and generate the right function call to get it."

**What I do:**

. Open the vLLM Playground web UI:
+
link:{vllm_playground_url}[Open vLLM Playground^]

. If a server is running, stop it first:
+
* Click *Stop Server* in the Server Configuration panel

. In the *Server Configuration* panel, configure tool calling:
+
.. Check *Enable Tool Calling*
.. Select a tool calling parser (or leave as "Auto-detect")
image::module-03-figure-01.png[vLLM Playground Server Configuration panel showing Enable Tool Calling checkbox checked in blue, Tool Call Parser dropdown menu set to Hermes parser, and Model dropdown displaying Qwen/Qwen2.5-3B-Instruct selected. The Start Server button is visible below the configuration options ready to launch the tool-calling enabled server,align="center",width=500,link=module-03-figure-01.png^,title="Enable Tool Calling"]
+
.. Choose a model that supports tool calling

. For this configuration, use:
+
[cols="1,2"]
|===
|Setting |Value

|Model
|`Qwen/Qwen2.5-3B-Instruct` (or similar Qwen model)

|Enable Tool Calling
|Checked

|Tool Call Parser
|`hermes` (or Auto-detect)
|===
+
NOTE: Qwen models are open and don't require HuggingFace authentication. If using gated models like Llama, ensure HuggingFace access is configured.

. Click *Start Server* and wait for the model to load.

. Monitor the server logs in the *Server Logs* panel for tool calling confirmation. Look for messages indicating tool calling is enabled and the model has loaded successfully.

**What they should notice:**

* The "Enable Tool Calling" checkbox activates parser configuration options
* Different parser options correspond to different model families (Llama, Mistral, Hermes)
* Server logs confirm tool calling parser loaded successfully

**Understanding tool calling parsers:**

Different model families use different formats for tool calling. vLLM supports several parsers:

[cols="1,2,2"]
|===
|Parser |Models |Format

|`llama3_json`
|Llama 3.x, Llama 3.1, Llama 3.2
|JSON-based function calls

|`mistral`
|Mistral, Mixtral
|Mistral's native tool format

|`hermes`
|Hermes, Qwen (with Hermes prompt)
|Hermes-style function calling

|`auto` (Auto-detect)
|Various
|Attempts to detect model type
|===

NOTE: **Tool calling does NOT mean LLM executes tools.** When tool calling is enabled, the AI model generates a JSON-formatted function call with arguments, but the LLM itself does not execute any code or call external APIs. The application (vLLM Playground in this case) is responsible for parsing the tool call, executing the actual function, and returning results to the model. This design ensures security and gives full control over what actions are taken.

**What I say:**

"With the server configured, let me define the tools that the AI can invoke. Each tool has a name, description, and parameter schema that tells the AI when and how to use it."

**What I do:**

. In vLLM Playground, navigate to the *Tools* panel (wrench icon in the toolbar).
+
image::module-03-figure-02.png[vLLM Playground Tools panel with wrench icon in the toolbar. The panel displays a JSON editor area for defining function calling tools with syntax highlighting, Add Tool button at top-left, Save button at bottom-right, and placeholder text for entering tool definitions in OpenAI-compatible function format,align="center",width=600,link=module-03-figure-02.png^,title="Tools Panel"]

. Click *Add Tool* to define the first customer support function.

. Create an order status lookup tool:
+
[source,json]
----
{
  "type": "function",
  "function": {
    "name": "get_order_status",
    "description": "Look up the current status of a customer order. Use this when a customer asks about their order, shipping, or delivery.",
    "parameters": {
      "type": "object",
      "properties": {
        "order_id": {
          "type": "string",
          "description": "The order ID to look up (e.g., ORD-12345)"
        }
      },
      "required": ["order_id"]
    }
  }
}
----
+
image::module-03-figure-03.png[JSON tool definition in the Tools panel showing get_order_status function with name field, description field explaining when to use this tool for customer order inquiries, and parameters object defining order_id as required string field with descriptive documentation. The JSON structure demonstrates proper OpenAI-compatible function calling format with type, function, parameters, and required fields clearly specified,align="center",width=600,link=module-03-figure-03.png^,title="Order Status Lookup Tool"]

. Add a second tool for customer information:
+
[source,json]
----
{
  "type": "function",
  "function": {
    "name": "get_customer_info",
    "description": "Retrieve customer account information. Use this when you need to verify customer identity or look up account details.",
    "parameters": {
      "type": "object",
      "properties": {
        "customer_email": {
          "type": "string",
          "description": "Customer's email address"
        },
        "customer_id": {
          "type": "string",
          "description": "Customer's account ID (optional if email provided)"
        }
      },
      "required": ["customer_email"]
    }
  }
}
----

. Add a third tool for scheduling callbacks:
+
[source,json]
----
{
  "type": "function",
  "function": {
    "name": "schedule_callback",
    "description": "Schedule a callback from a support agent. Use this when the customer requests to speak with a human or needs escalated support.",
    "parameters": {
      "type": "object",
      "properties": {
        "customer_phone": {
          "type": "string",
          "description": "Customer's phone number for callback"
        },
        "preferred_time": {
          "type": "string",
          "description": "Preferred callback time (e.g., 'morning', 'afternoon', '2pm EST')"
        },
        "issue_summary": {
          "type": "string",
          "description": "Brief summary of the customer's issue"
        }
      },
      "required": ["customer_phone", "issue_summary"]
    }
  }
}
----

. Add a product search tool:
+
[source,json]
----
{
  "type": "function",
  "function": {
    "name": "search_products",
    "description": "Search the product catalog. Use this when a customer asks about products, availability, or pricing.",
    "parameters": {
      "type": "object",
      "properties": {
        "query": {
          "type": "string",
          "description": "Search query (product name, category, or keywords)"
        },
        "category": {
          "type": "string",
          "description": "Product category filter (optional)",
          "enum": ["electronics", "clothing", "home", "sports", "all"]
        },
        "max_results": {
          "type": "integer",
          "description": "Maximum number of results to return"
        }
      },
      "required": ["query"]
    }
  }
}
----

. The tools panel should now show 4 defined tools:
+
image::module-03-figure-04.png[Tools panel displaying four customer support tools defined: get_order_status, get_customer_info, schedule_callback, and search_products. Each tool shows its function name, description, and parameter schema. The list view shows all tools are successfully saved and ready for the AI model to invoke during customer support conversations,align="center",width=600,link=module-03-figure-04.png^,title="Defined Tools"]

**What they should notice:**

* Each tool has a clear name, description, and parameter schema
* Descriptions guide the AI on *when* to use each tool
* Parameters define *what* information the AI needs to extract from the conversation
* Required vs optional parameters give the AI flexibility

**Best practices for tool definitions:**

[cols="1,2"]
|===
|Practice |Why It Matters

|Clear descriptions
|Helps AI decide when to use the tool

|Specific parameter names
|Reduces ambiguity in extracted values

|Required vs optional
|Guides AI on minimum needed information

|Enum constraints
|Limits values to valid options

|Default values
|Provides sensible fallbacks
|===

**If asked:**

Q: "Why use Hermes parser instead of Auto-detect?"
A: "Auto-detect works for most cases, but explicitly setting the parser avoids misidentification. Qwen models work best with the Hermes parser."

Q: "Can we define tools dynamically at runtime?"
A: "Yes. Tools are passed via the API request body, so they can be changed per conversation. In production, different support tiers could have different tool sets."

Q: "How many tools can we define?"
A: "There is no hard limit, but more tools means more context for the model to evaluate. Best practice is to keep tools focused and relevant to the use case, typically under 10-15 tools."

[[part-2]]
== Part 2 - Advanced Tool Patterns (Optional)

=== Know

_Customer challenge: ACME needs confidence that tool calling works reliably across diverse customer interactions. Support conversations range from simple order lookups to complex multi-step requests, and the AI must handle all of them correctly._

**Business Impact:**

* Incorrect tool calls waste backend resources and return wrong data to customers
* Failure to invoke tools when needed forces fallback to manual support
* Multi-step requests (check order AND search products) require the AI to handle multiple tools in a single turn
* Conversational messages that do not need tools must still receive natural responses

**Value Proposition:**

Testing the tool calling workflow across multiple scenarios validates that the AI correctly identifies when to use tools, extracts the right arguments, and responds naturally when tools are not needed. This builds confidence before connecting to live backend systems in Module 4.

=== Show

**What I say:**

"Let me walk through several real customer scenarios to show how the AI decides when and which tools to call. Watch how it extracts the right information from natural language."

**What I do:**

. Ensure the tools from Part 1 are loaded and the server is running with tool calling enabled.

. In the *Chat* panel, set a system prompt for customer support:
+
----
You are a helpful customer support assistant for ACME Corporation. You have access to tools to help customers with their orders, account information, scheduling callbacks, and product searches. Use the appropriate tool when a customer needs specific information or actions.
----

. In the *Tools* panel, set *Tool Choice* to *Auto (recommended)*.
+
This setting allows the AI to automatically decide when to use tools based on the conversation context.

. Test order status inquiry:
+
----
Hi, I placed an order last week and haven't received any updates. My order number is ORD-78432. Can you check the status?
----
+
Expected AI behavior, generates a tool call:
+
[source,json]
----
{
  "name": "get_order_status",
  "arguments": {
    "order_id": "ORD-78432"
  }
}
----
+
image::module-03-figure-05.png[Chat interface showing AI response generating a JSON-formatted tool call for get_order_status function with order_id argument set to ORD-12345 extracted from the customer query. The response demonstrates the AI correctly identifying the need to look up external data rather than fabricating order information,align="center",width=700,link=module-03-figure-05.png^,title="Tool Call Generated"]
+
NOTE: Streaming is disabled for tool calling due to a bug in vLLM v0.11.0. This issue has been resolved in later versions of vLLM.

. Point out the response format:
* The AI recognizes the need for external data
* Instead of making up information, it generates a function call
* The arguments are extracted from the customer's message

. Test customer lookup:
+
----
I need to update my shipping address. My email is john.smith@email.com
----
+
Expected tool call:
+
[source,json]
----
{
  "name": "get_customer_info",
  "arguments": {
    "customer_email": "john.smith@email.com"
  }
}
----
+
image::module-03-figure-06.png[Chat interface showing AI response generating a JSON tool call for get_customer_info function with customer_email argument populated with john.smith@email.com extracted from the customer message about updating shipping address,align="center",width=700,link=module-03-figure-06.png^,title="Customer Lookup Tool Call"]

. Test callback scheduling:
+
----
This is frustrating! I've been trying to resolve this billing issue for days. Can someone call me back? My number is 555-123-4567, preferably in the afternoon.
----
+
Expected tool call:
+
[source,json]
----
{
  "name": "schedule_callback",
  "arguments": {
    "customer_phone": "555-123-4567",
    "issue_summary": "Billing issue",
    "preferred_time": "afternoon"
  }
}
----
+
image::module-03-figure-07.png[Chat interface showing AI response generating a JSON tool call for schedule_callback function with customer_phone (555-123-4567), issue_summary (Billing issue), and preferred_time (afternoon) arguments extracted from frustrated customer message requesting callback,align="center",width=700,link=module-03-figure-07.png^,title="Callback Scheduling Tool Call"]

. Test product search:
+
----
Do you have any wireless headphones under $100?
----
+
Expected tool call:
+
[source,json]
----
{
  "name": "search_products",
  "arguments": {
    "query": "wireless headphones",
    "category": "electronics",
    "max_results": 5
  }
}
----
+
image::module-03-figure-08.png[Chat interface showing AI response generating a JSON tool call for search_products function with query (wireless headphones), category (electronics), and max_results (5) arguments extracted from customer inquiry about headphones under $100,align="center",width=700,link=module-03-figure-08.png^,title="Product Search Tool Call"]

. Test a message that does not need tools:
+
----
Thanks for your help today!
----
+
Expected: Normal text response (no tool call). The AI should respond conversationally without invoking a tool.

. Test multiple potential tools:
+
----
I want to check on order ORD-99001 and also see if you have the new laptop model in stock.
----
+
Observe: The AI may generate multiple tool calls or prioritize one. Different models handle this differently.
+
image::module-03-figure-09.png[Chat interface showing AI response generating multiple JSON tool calls in a single reply: get_order_status for checking order ORD-99001 and search_products for finding new laptop model. Demonstrates AI ability to handle complex queries requiring multiple external data lookups simultaneously,align="center",width=700,link=module-03-figure-09.png^,title="Multiple Tool Calls"]

**What they should notice:**

* AI generates tool calls for appropriate requests and responds normally when tools are not needed
* Arguments are correctly extracted from natural language customer messages
* Tool names match the defined functions
* JSON format is valid and parseable
* The AI does not fabricate data. It asks the backend for real information

**Understanding the workflow:**

[source,text]
----
+---------------------------------------------------------+
|                    Tool Calling Workflow                 |
|                                                         |
|  1. Customer Message                                    |
|     "Check order ORD-12345"                             |
|              |                                          |
|  2. AI Analysis                                         |
|     Recognizes need for get_order_status tool           |
|              |                                          |
|  3. Tool Call Generated                                 |
|     {"name": "get_order_status", "arguments": {...}}    |
|              |                                          |
|  4. [Module 4] Tool Execution    (MCP Server)           |
|     Actual function runs, returns data                  |
|              |                                          |
|  5. [Module 4] AI Response                              |
|     AI incorporates result into customer response       |
+---------------------------------------------------------+
----

In this module, steps 1-3 were demonstrated. Module 4 (MCP Integration) covers steps 4-5 with actual tool execution.

**If asked:**

Q: "What happens if the AI calls the wrong tool?"
A: "Good tool descriptions minimize this. In production, you would add validation logic before executing any tool call. The AI's tool call is just a suggestion that your application decides whether to execute."

Q: "Can the AI chain multiple tool calls together?"
A: "Yes, some models support parallel tool calling. The AI can generate multiple tool calls in a single response, and the application can execute them and return all results for the AI to synthesize."

Q: "How does this compare to LangChain or other agent frameworks?"
A: "This is the same underlying mechanism. LangChain, CrewAI, and other frameworks use OpenAI-compatible tool calling under the hood. vLLM provides the native inference support that those frameworks build on."

== Clean up

Before proceeding to Module 4, stop the current vLLM server to prepare for MCP integration:

. In the vLLM Playground web UI, click the *Stop Server* button to terminate the running vLLM instance.

. Verify the server has stopped by checking that the server status shows "Stopped" or the Start Server button becomes available again.

NOTE: Module 4 requires restarting the vLLM Playground daemon service after installing MCP dependencies. Stopping the server now ensures a clean transition.

== Troubleshooting

**Issue**: "Tool calling not supported for this model"

**Solution**:

. Use a model that supports function calling (Llama 3.x, Mistral, Qwen)
. Check the model's documentation for tool calling support
. Try a different parser setting

**Issue**: Server fails to start with tool calling enabled

**Solution**:

. Check GPU memory. Tool calling may require additional resources
. Try a smaller model
. Review server logs for specific errors

**Issue**: Tool calls not appearing in response

**Solution**:

. Verify "Enable Tool Calling" is checked
. Restart server after enabling
. Check server logs for tool-related errors

**Issue**: AI responds with text instead of tool calls

**Solution**:

. Verify tool calling is enabled in server config
. Check that tools are properly loaded
. Ensure system prompt mentions available tools
. Try rephrasing the customer message more explicitly

**Issue**: AI generates wrong tool or arguments

**Solution**:

. Improve tool descriptions to be more specific
. Add examples in the description
. Check parameter names are clear and unambiguous
. Try a different bigger model (Llama 4 has improved tool calling)

**Issue**: Invalid JSON in tool call

**Solution**:

. Check tool call parser matches model
. Try "Auto-detect" parser setting
. Some models may need specific prompt formatting

**Issue**: Model ignores defined tools

**Solution**:

. Ensure tools are saved/loaded before chatting
. Add system prompt explicitly mentioning tools
. Use models known for good tool calling (Llama 3.1+, Mistral)

**Issue**: Performance degradation with tools

**Solution**:

. Tool calling adds processing overhead
. Reduce number of tools if possible
. Use concise tool descriptions

== Module summary

**What was demonstrated:**

* Enabled tool calling in vLLM Playground server configuration
* Covered tool calling parsers for Llama, Mistral, and Hermes models
* Defined 4 customer support tools with proper schemas
* Tested tool calling workflow with various customer scenarios
* Observed AI-generated function calls with extracted arguments
* Showed the difference between tool call generation and tool execution

**Key takeaways:**

* Tool calling transforms AI from passive responder to active assistant
* Good tool descriptions are critical for AI decision-making
* The AI generates structured calls; backend systems handle execution
* Different models have different tool calling capabilities
* Tool calling is the foundation for agentic AI workflows

**Business impact for ACME:**

* AI can now recognize when it needs customer data
* Structured tool calls integrate with existing backend APIs
* Reduces need for customers to provide information multiple ways
* Foundation for automated support workflows

**Next module:**

Module 4 explores *Advanced Inferencing: MCP Integration*, connecting the AI to actual external tools with human-in-the-loop approval for safe execution.

== References

* link:https://github.com/micytao/vllm-playground[vLLM Playground GitHub Repository^] - Tool calling configuration and usage
* link:https://github.com/vllm-project/vllm[vLLM Project^] - High-performance inference engine
* link:https://docs.vllm.ai/en/latest/features/tool_calling.html[vLLM Tool Calling Documentation^] - Guided decoding and tool calling reference
