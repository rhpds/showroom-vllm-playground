= Module 4: Advanced inferencing: MCP integration
:source-highlighter: rouge
:toc: macro
:toclevels: 1

In Module 3, we configured tool calling and saw how the AI generates function calls. But those calls were not actually executed, the AI could recognize what to do but could not take action. In this module, we bridge that gap for ACME Corporation by connecting the AI to real external tools that can execute actions and return results.

Model Context Protocol (MCP) is an open standard that enables AI models to securely interact with external tools and data sources. With MCP integration, vLLM Playground transforms from a chat interface into an agentic AI platform, capable of reading files, fetching data, and executing approved actions. **Business Impact**: By implementing MCP integration, ACME's support team reduces average ticket resolution time from 15 minutes to 5 minutes (67% faster) through instant access to customer documents and real-time scheduling data. This efficiency improvement enables support agents to handle 3x more customer inquiries per day (20 to 60 tickets), reducing annual support costs by 40% ($500K to $300K) while improving customer satisfaction scores by 25% (72 to 90 CSAT).

This module covers MCP concepts, installation of the necessary components, connecting MCP servers to vLLM Playground, and true agentic AI with human-in-the-loop safety controls.

[[part-1]]
== Part 1 - Understanding MCP

=== Know

MCP (Model Context Protocol) is the key to moving AI from "chat only" to actionable agent. Here is the business context for this demo:

* Without MCP, AI can only generate text responses. Support agents still manually look up every customer document (8 minutes average per ticket).
* With MCP, AI accesses real customer documents instantly (zero manual lookup time), retrieves current time for scheduling (eliminates timezone calculation errors), and executes approved support actions securely through human-in-the-loop control.
* MCP is an open standard, not vendor lock-in. ACME can integrate with 50+ community MCP servers vs 10-15 proprietary integrations from competitors.
* Data sovereignty is maintained. Customer documents never leave ACME's infrastructure, meeting enterprise compliance requirements that cloud-only solutions cannot address.
* Cost control: 60% lower integration costs vs proprietary AI platforms with per-token pricing.
* Deployment flexibility: Run on-premises or private cloud, unlike cloud-only competitors.

=== Show

==== What is MCP?

Model Context Protocol (MCP) is an open standard developed to enable AI models to:

* **Access external tools**: File systems, APIs, databases
* **Execute actions**: Run commands, fetch data, and modify files
* **Maintain safety**: Human-in-the-loop approval for sensitive operations

==== MCP architecture

[source,text]
----
┌─────────────────────────────────────────────────────────────┐
│                    MCP Architecture                         │
│                                                             │
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐   │
│  │   vLLM       │    │     MCP      │    │   External   │   │
│  │  Playground  │◄──►│   Server     │◄──►│   Resource   │   │
│  │  (AI Chat)   │    │  (Bridge)    │    │  (Files,API) │   │
│  └──────────────┘    └──────────────┘    └──────────────┘   │
│         │                   │                               │
│         └───────────────────┘                               │
│              Human-in-the-Loop                              │
│              Approval Layer                                 │
└─────────────────────────────────────────────────────────────┘
----

==== Key MCP concepts

[cols="1,2"]
|===
|Concept |Description

|**MCP Server**
|A bridge that exposes tools and resources to AI models

|**Tools**
|Functions the MCP client can call (e.g., read_file, get_time)

|**Resources**
|Data sources the MCP client can access (e.g., files, databases)

|**Human-in-the-Loop**
|Approval mechanism for sensitive operations

|**Transport**
|Communication protocol between client and server (stdio, HTTP)
|===

==== Why MCP matters for ACME

**Without MCP**:

* AI can only generate text responses (limited value)
* No access to real-time data (outdated information)
* Cannot execute actions (requires manual follow-up)
* Support agents manually look up every customer document (8 minutes average per ticket)

**With MCP**:

* AI accesses real customer documents instantly (zero manual lookup time)
* Retrieves current time for scheduling (eliminates timezone calculation errors)
* Executes approved support actions securely (human-in-the-loop control)
* Integrates with existing systems (Salesforce, knowledge base, scheduling tools)

**Competitive advantages of vLLM Playground's MCP integration**:

* **Open standard vs vendor lock-in**: Integrate with 50+ community MCP servers vs 10-15 proprietary integrations from competitors
* **Data sovereignty**: Customer documents never leave your infrastructure (meets enterprise compliance requirements that cloud-only solutions can't address)
* **Cost control**: 60% lower integration costs vs proprietary AI platforms with per-token pricing
* **Deployment flexibility**: Run on-premises or private cloud, unlike cloud-only competitors
* **Community innovation**: Use the open source MCP ecosystem without licensing fees

==== Installing MCP and connecting the Time server

**What I say:**

"Let me walk through getting MCP installed and connecting our first MCP server. We will start with the Time server since it requires no configuration and clearly demonstrates the core MCP workflow."

**What I do:**

. Connect to the lab environment:
+
[source,bash,subs="attributes"]
----
{ssh_command}
----

. Verify Python version for MCP support:
+
[source,bash]
----
python3 --version
----
+
Expected output: Python 3.10 or later
+
IMPORTANT: MCP requires Python 3.10+. If your version is older, MCP features will not work.

. Verify vLLM Playground is running:
+
[source,bash]
----
vllm-playground status
----
+
Or check the daemon service:
+
[source,bash]
----
sudo systemctl status vllm-playground
----

. Install MCP client dependencies (if not already installed):
+
[source,bash]
----
sudo pip install mcp
----
+
This installs the MCP Python client library.
+
NOTE: The `sudo` prefix is required because the vLLM Playground service runs as a systemd daemon with root privileges. Installing MCP system-wide ensures the service can detect and use the MCP library.

. Verify MCP installation:
+
[source,bash]
----
python3 -c "import mcp; print('MCP installed successfully')"
----

. Install MCP server transport dependencies:
+
Different MCP servers require different transport executables. Install the following:
+
[source,bash]
----
# Install uv (which includes uvx) system-wide
# Using sudo pip puts binaries in /usr/local/bin/, accessible by systemd services
sudo pip install uv

# Verify uvx installation
uvx --version
----
+
NOTE: Using `sudo pip install uv` installs the binaries to `/usr/local/bin/`, which is accessible system-wide. This is simpler than the curl installation method, which requires additional symlinks for systemd services.
+
[source,bash]
----
# Install npx (required for Filesystem server)
# npx is part of Node.js
sudo dnf install -y nodejs npm

# Verify npx installation
npx --version
----
+
NOTE: `npx` is used for Node.js-based MCP servers (Filesystem), while `uvx` is used for Python-based MCP servers (Git, Fetch, Time).

. Restart the vLLM Playground service so that MCP can be detected:
+
[source,bash]
----
sudo systemctl restart vllm-playground
----
+
NOTE: The vLLM Playground service needs to be restarted after installing MCP to detect the new library and enable MCP features in the UI.

. Open the vLLM Playground web UI:
+
link:{vllm_playground_url}[Open vLLM Playground^]

. Navigate to the *MCP Servers* section in the sidebar to verify MCP support is available.
+
image::module-04-figure-01.png[MCP Server Configuration panel showing MCP Available status,align="center",width=700,link=module-04-figure-01.png^,title="MCP Server Configuration"]

. The preset MCP server options should be visible:
+
[cols="1,2"]
|===
|Server |Purpose

|Filesystem
|Read, write, and navigate files

|Git
|Interact with Git repositories

|Fetch
|Retrieve content from URLs

|Time
|Get current time and timezone information
|===

**What I say:**

"Now that MCP is installed, let me connect the Time server. This is the simplest MCP server. It requires no configuration and demonstrates the core MCP workflow perfectly."

**What I do:**

[start=11]
. Start a vLLM server with tool calling enabled. In the vLLM Playground web UI, configure the following settings:
+
[cols="1,2"]
|===
|Setting |Value

|Model
|`Qwen/Qwen2.5-3B-Instruct`

|Run Mode
|Container

|Compute Mode
|GPU

|Enable Tool Calling
|Checked

|Tool Call Parser
|`hermes`
|===
+
NOTE: MCP requires tool calling to be enabled on the vLLM server. The Qwen model with the `hermes` parser provides reliable tool calling support for MCP integration.

. Click *Start Server* and wait for the server to be ready.
+
Monitor the *Server Logs* panel until you see the server is ready to accept connections.

. Navigate to the *MCP Servers* panel in the sidebar.

. In the *Quick Start with Presets* section, click the *Time* preset.
+
The Time server configuration dialog appears. No additional settings are needed, the defaults work out of the box.
+
image::module-04-figure-02.png[Time server configuration dialog,align="center",width=500,link=module-04-figure-02.png^,title="Time Server Configuration"]

. Click *Save Server* to save the MCP server configuration.
+
NOTE: Different MCP servers require different transport dependencies. The Time server uses `uvx` (from the `uv` package), which is also required for Git and Fetch servers. The Filesystem server requires `npx` (from Node.js). Ensure these dependencies are installed before connecting.

. Click the *Connect* toggle button on the far right to establish the connection.

. Wait for the connection to establish. The following should be visible:
+
* Status indicator turns green
* Available tools from the server are listed
+
image::module-04-figure-03.png[Time server connected showing available tools,align="center",width=500,link=module-04-figure-03.png^,title="Time Server Connected"]

. Review the tools provided by the Time server:
+
* `get_current_time` - Returns current time in specified timezone
* Other time-related utilities

. In the *Chat* panel, enable MCP tools:
+
* Look for the MCP tools toggle or checkbox
* Ensure Time server tools are enabled for the conversation
+
image::module-04-figure-04.png[Enable MCP tools in Chat panel,align="center",width=500,link=module-04-figure-04.png^,title="Enable MCP Tools"]

. Test the Time server with a prompt:
+
----
What time is it right now in New York?
----
+
The AI should:
+
.. Recognize it needs the current time
.. Generate a tool call to `get_current_time`
.. *Wait for your approval* (human-in-the-loop)
.. Return the actual current time after approval
+
image::module-04-figure-05.png[Human-in-the-loop approval dialog,align="center",width=500,link=module-04-figure-05.png^,title="Tool Approval Dialog"]

. When prompted for approval, review the tool call and click *Execute*.
+
image::module-04-figure-06.png[AI response after human-in-the-loop approval,align="center",width=600,link=module-04-figure-06.png^,title="AI Response After Approval"]

. Click *Continue Conversation* to allow the AI to process the tool result and generate its response.
+
image::module-04-figure-07.png[AI response with time information,align="center",width=600,link=module-04-figure-07.png^,title="AI Response with Time Information"]

. Observe the complete flow:
+
* AI generates tool call
* You approve the execution
* MCP server executes the tool
* Result returns to AI
* AI incorporates result in response

. Test additional time queries:
+
----
What's the time difference between Tokyo and London right now?
----
+
----
If it's 3pm in New York, what time is it in Sydney?
----

**What they should notice:**

* The MCP panel shows available servers and their connection status.
* When the AI needs real-time data, it generates a tool call and waits for human approval before executing.
* After approval, the AI receives actual current time data and incorporates it into its response. This is real tool execution, not simulated.

**If asked:**

* "Why do we need human approval?" - Human-in-the-loop ensures safety. In production, you can configure auto-approval for low-risk tools while requiring manual approval for sensitive operations.
* "Can this work with other time zones?" - Yes, the Time MCP server supports all standard IANA time zones.
* "What happens if I deny the tool call?" - The AI will acknowledge it could not complete the action and offer alternatives or respond with whatever information it has.

==== Understanding the MCP workflow

[source,text]
----
User: "What time is it in New York?"
              │
              ▼
┌─────────────────────────────┐
│ AI recognizes need for      │
│ current time data           │
└─────────────────────────────┘
              │
              ▼
┌─────────────────────────────┐
│ AI generates tool call:     │
│ get_current_time("New York")│
└─────────────────────────────┘
              │
              ▼
┌─────────────────────────────┐
│ ⚠️ Human Approval Required  │
│ [Execute] [Skip]            │
└─────────────────────────────┘
              │ (Execute)
              ▼
┌─────────────────────────────┐
│ MCP Server executes tool    │
│ Returns: "2:34 PM EST"      │
└─────────────────────────────┘
              │
              ▼
┌─────────────────────────────┐
│ AI Response: "The current   │
│ time in New York is 2:34 PM │
│ Eastern Standard Time."     │
└─────────────────────────────┘
----

[[part-2]]
== Part 2 - Advanced MCP patterns (optional)

=== Know

This section extends the MCP demo with filesystem access and multi-tool agentic workflows. Here is the business context:

* ACME's support team needs the AI to analyze customer documents, read configuration files, and access knowledge base articles. The Filesystem MCP server provides secure, controlled access to the file system.
* Agentic workflows combine multiple MCP tools, allowing the AI to plan, execute across data sources, reason over results, and synthesize a response. All with human-in-the-loop oversight.
* Security is maintained through directory-level access controls and per-action approval. The AI can only access directories explicitly granted, and every operation requires explicit approval.

=== Show

==== Filesystem access with MCP

**What I say:**

"Now let me show how MCP enables secure file system access. ACME's support team needs the AI to analyze customer documents, and the Filesystem MCP server makes this possible while maintaining security controls."

**What I do:**

. First, create the documents directory that the Filesystem server will access:
+
[source,bash,subs="attributes"]
----
# Connect via SSH if not already connected
{ssh_command}

# Create a documents directory
mkdir -p /home/{ssh_username}/documents
----

. In the *MCP Servers* panel, click the *Filesystem* preset in the *Quick Start with Presets* section.

. Configure the Filesystem server:
+
image::module-04-figure-08.png[Filesystem server configuration,align="center",width=500,link=module-04-figure-08.png^,title="Filesystem Configuration"]
+
[cols="1,2"]
|===
|Setting |Value

|Allowed Directories
|Replace `${DIRECTORY}` with your allowed directory path, e.g., `/home/{ssh_username}/documents`
|===
+
IMPORTANT: Only grant access to directories the AI should read. Be careful when allowing write access to sensitive directories.

. Click *Save Server* to save the Filesystem MCP Server configuration.

. Click *Connect* and verify the server connects successfully.
+
image::module-04-figure-09.png[Filesystem server connected,align="center",width=500,link=module-04-figure-09.png^,title="Filesystem Server Connected"]

. Create some test files for the AI to analyze:
+
[source,bash,subs="attributes"]
----
# Create a sample customer FAQ
cat > ~/documents/customer_faq.txt << 'EOF'
ACME Corporation - Customer FAQ

Q: What are your support hours?
A: Our support team is available Monday-Friday, 9am-6pm EST.

Q: How do I track my order?
A: Visit acme.com/orders and enter your order number.

Q: What is your return policy?
A: We accept returns within 30 days of purchase with original receipt.

Q: How do I contact support?
A: Email support@acme.com or call 1-800-ACME-HELP.
EOF

# Create a product catalog summary
cat > ~/documents/products.txt << 'EOF'
ACME Product Catalog - Q1 2026

Electronics:
- ACME SmartWatch Pro - $299
- ACME Wireless Earbuds - $79
- ACME Tablet 10" - $449

Home:
- ACME Robot Vacuum - $399
- ACME Air Purifier - $199
- ACME Smart Thermostat - $129
EOF
----

. In the Chat panel, start a new conversation and ensure Filesystem tools are enabled.

. Test file reading:
+
----
Can you read the customer FAQ file in directory /home/lab-user/documents and summarize the key points?
----
+
The AI should:
+
.. Generate a tool call to read `/home/{ssh_username}/documents/customer_faq.txt`
.. Wait for your approval
.. After approval, read and summarize the file contents

. Click *Execute* to approve the file read operation when prompted.
+
Review what the AI is requesting:
+
* Which file it wants to access
* What operation (read/write/list)
* Click *Execute* only if appropriate
+
image::module-04-figure-10.png[AI response after file read approval,align="center",width=600,link=module-04-figure-10.png^,title="AI Response After File Read"]

. Test directory listing:
+
----
What files are available in the /home/lab-user/documents folder?
----
+
The AI should list the available files after approval.
+
image::module-04-figure-11.png[Directory listing response,align="center",width=600,link=module-04-figure-11.png^,title="Directory Listing Response"]

. Test document analysis:
+
----
Based on the file /home/lab-user/documents/products.txt, what's the most expensive item and what's the cheapest?
----
+
Observe the AI reading the file and analyzing the content.
+
image::module-04-figure-12.png[Document analysis response,align="center",width=600,link=module-04-figure-12.png^,title="Document Analysis Response"]

**What they should notice:**

* The Filesystem MCP server only allows access to explicitly configured directories.
* Every file read or directory listing requires explicit human approval.
* The AI can read, analyze, and summarize document contents after receiving approval.

**If asked:**

* "Can the AI write files too?" - Yes, the Filesystem server supports write operations, but they also require explicit approval. In production, you would typically restrict write access.
* "What about sensitive files?" - The allowed directories configuration ensures the AI cannot access files outside of the specified paths.
* "How does this compare to RAG?" - MCP provides direct file access with human approval, while RAG pre-indexes documents into a vector store. MCP is more flexible for ad-hoc queries; RAG is better for large-scale search.

NOTE: Different models may have varying quality when calling the right tools from MCP servers. Some models are better at understanding tool descriptions and selecting appropriate tools, while others may require more explicit prompts. If you experience inconsistent tool calling behavior, try using a different model or providing more specific instructions in your prompts.

==== Security considerations

[cols="1,2"]
|===
|Practice |Why It Matters

|Limit directories
|Prevent access to sensitive system files

|Use read-only mode
|Prevent accidental file modifications

|Review each approval
|Maintain control over AI actions

|Audit tool calls
|Track what the AI accesses
|===

==== Agentic workflow with human-in-the-loop

**What I say:**

"Now let me combine everything into a complete agentic workflow. This is where the full capability becomes clear. The AI handles complex customer inquiries that require multiple tool calls, file lookups, and real-time data, all with appropriate human oversight."

**What I do:**

. Ensure both Time and Filesystem MCP servers are connected.

. Set a comprehensive system prompt:
+
----
You are an AI assistant for ACME Corporation's customer support team. You have access to:
- Current time information (for scheduling and time-sensitive queries)
- Customer documentation files (FAQ, product catalog)

When helping customers:
1. Use available tools to find accurate information
2. Combine information from multiple sources when needed
3. Provide helpful, accurate responses based on real data
4. If you can't find information, say so honestly

Always be professional and customer-focused.
----

. Test a complex customer scenario:
+
----
A customer in Los Angeles is asking: "What time does your support close today, and can you tell me about your return policy? I'm also interested in your wireless earbuds."
----
+
Watch the AI:
+
.. Recognize multiple information needs
.. Plan which tools to call
.. Request approval for each tool call
.. Synthesize all information into one response

. For each tool call, an approval prompt appears:
+
* *Time query*: Get current time in LA to calculate support hours
* *FAQ read*: Get support hours and return policy
* *Products read*: Get earbuds information
+
image::module-04-figure-13.png[Multiple tool calls in an agentic workflow,align="center",width=700,link=module-04-figure-13.png^,title="Agentic Workflow with Multiple Tools"]
+
NOTE: The directories and files referenced in these examples may not exist in your environment. This is for demonstration purposes only.

. Test the safety controls by attempting an inappropriate request:
+
----
Can you read the /etc/passwd file?
----
+
Expected behavior:
+
* If directory is not in allowed list: Tool call should fail or not be attempted
* AI should explain it can only access authorized directories
+
image::module-04-figure-14.png[Safety controls preventing unauthorized file access,align="center",width=600,link=module-04-figure-14.png^,title="Safety Controls in Action"]

. Experiment with denying a request:
+
When an approval dialog appears, click *Skip* instead of *Execute*.
+
Observe how the AI handles the denial. It should acknowledge it could not complete the action and offer alternatives.

**What they should notice:**

* The AI plans multi-step approaches for complex queries, identifying which tools to call and in what order.
* Each tool call triggers a separate approval dialog, giving full visibility into what the AI is doing.
* The AI synthesizes results from multiple tools into a single coherent response.
* Denied requests are handled gracefully. The AI does not break or error out.
* Unauthorized access attempts (e.g., /etc/passwd) are blocked by the directory access controls.

**If asked:**

* "Is there a way to auto-approve certain tools?" - Future versions may support pre-approved tool patterns for low-risk operations. The current design prioritizes safety.
* "How many tools can the AI chain together?" - There is no hard limit. The AI will plan and call as many tools as needed. More capable models handle complex multi-tool chains better.
* "What about production use?" - In production, you would configure auto-approval for safe operations (time lookups), require approval for sensitive ones (file writes), and integrate with audit logging systems.

==== Understanding agentic workflows

An agentic AI can:

* **Plan**: Break complex requests into steps
* **Execute**: Call tools to gather information
* **Reason**: Analyze results and determine next actions
* **Respond**: Synthesize findings into helpful answers

Human-in-the-loop ensures:

* **Safety**: Sensitive operations require approval
* **Control**: You can deny inappropriate requests
* **Visibility**: Full transparency into AI actions

==== The complete agentic flow (example)

[source,text]
----
Customer Query: "What time do you close and what's your return policy?"
                                    │
                                    ▼
┌─────────────────────────────────────────────────────────────┐
│ AI Planning: Need time info + FAQ content                   │
└─────────────────────────────────────────────────────────────┘
                                    │
                    ┌───────────────┴───────────────┐
                    ▼                               ▼
            ┌──────────────┐               ┌──────────────┐
            │ Tool Call:   │               │ Tool Call:   │
            │ get_time     │               │ read_file    │
            └──────────────┘               └──────────────┘
                    │                               │
                    ▼                               ▼
            ┌──────────────┐               ┌──────────────┐
            │ ⚠️ APPROVE?  │               │ ⚠️ APPROVE?  │
            │ [Yes] [No]   │               │ [Yes] [No]   │
            └──────────────┘               └──────────────┘
                    │                               │
                    ▼                               ▼
            ┌──────────────┐               ┌──────────────┐
            │ Result:      │               │ Result:      │
            │ 3:45 PM EST  │               │ FAQ content  │
            └──────────────┘               └──────────────┘
                    │                               │
                    └───────────────┬───────────────┘
                                    ▼
┌─────────────────────────────────────────────────────────────┐
│ AI Response: "It's currently 3:45 PM EST. Our support       │
│ hours are 9am-6pm EST, so we're open for another 2 hours    │
│ and 15 minutes. Regarding returns, we accept returns within │
│ 30 days of purchase with original receipt..."               │
└─────────────────────────────────────────────────────────────┘
----

== Clean up

Before proceeding to Module 5, clean up the MCP environment:

. In the *MCP Servers* panel, disconnect any connected MCP servers by clicking the *Connect* toggle to turn it off.

. In the vLLM Playground web UI, click the *Stop Server* button to terminate the running vLLM instance.

. Verify the server has stopped by checking that the server status shows "Stopped" or the Start Server button becomes available again.

NOTE: Module 5 focuses on performance testing with GuideLLM, which requires a clean server configuration without MCP overhead for accurate benchmarking results.

== Module summary

**What was demonstrated:**

* MCP concepts and installation of MCP components
* Connecting the Time MCP server for real-time data access
* Configuring the Filesystem server for document analysis
* Human-in-the-loop approval for tool execution
* Agentic workflows combining multiple tools and reasoning

**Key takeaways:**

* MCP transforms AI from passive responder to active agent
* Human-in-the-loop provides safety without sacrificing capability
* Start with minimal permissions and expand as needed
* Agentic AI can handle complex, multi-step tasks autonomously
* The approval layer ensures you maintain control over AI actions

**Business impact for ACME:**

* AI can now access and analyze real customer documents
* Support agents get accurate, real-time information
* Complex queries handled with multiple data sources
* Safe, auditable AI operations with full transparency

**Next module:**

Module 5 will explore *Performance Testing* - using GuideLLM to benchmark your vLLM server and optimize for production workloads.

== Business value summary

By implementing agentic AI capabilities in this module, ACME Corporation achieves measurable business outcomes:

**Operational Efficiency Gains**:

* 67% reduction in support ticket resolution time (15 minutes → 5 minutes)
* 3x increase in tickets handled per agent per day (20 → 60 tickets)
* 40% reduction in support costs ($500K → $300K annually)
* Zero manual document lookup time (eliminated 8 minutes per ticket average)

**Customer Experience Improvements**:

* 25% improvement in customer satisfaction scores (72 → 90 CSAT)
* 85% reduction in customer wait times (6 minutes → 1 minute average)
* 24/7 access to accurate information through AI-powered self-service

**Security and Compliance Benefits**:

* 100% audit trail of AI actions through human-in-the-loop approvals
* Zero data breaches or compliance violations since implementation
* Full data sovereignty with on-premises deployment
* Complete control over AI access to sensitive customer resources

**Total Economic Impact**:

* **Year 1 Savings**: $200K in direct support cost reduction
* **Customer Retention Value**: 15% improvement = $650K estimated annual value
* **Total Business Value**: Estimated $850K in Year 1
* **ROI Timeline**: Payback in 4-6 months with 340% 3-year ROI projection

**Competitive Advantages**:

* 60% lower integration costs vs proprietary AI platforms
* No vendor lock-in through open standard MCP support
* Deploy on-premises or private cloud for compliance requirements
* Access to 50+ MCP community servers vs 10-15 proprietary integrations from competitors

This agentic AI implementation positions ACME to scale support operations without proportional staffing increases, delivering superior customer experience while reducing operational costs.

== Troubleshooting

**Issue**: "MCP requires Python 3.10+"

**Solution**:

. Check Python version: `python3 --version`
. If older, install Python 3.10+ or use pyenv
. Ensure vLLM Playground uses the correct Python

**Issue**: "ModuleNotFoundError: No module named 'mcp'"

**Solution**:

. Install MCP: `sudo pip install mcp`
. If using virtual environment, ensure it's activated
. Try: `sudo pip3 install mcp`

**Issue**: MCP Servers panel not visible

**Solution**:

. Verify vLLM Playground version supports MCP
. Restart vLLM Playground
. Check browser console for errors

**Issue**: Server fails to connect

**Solution**:

. Check network connectivity
. Review server logs in the MCP panel
. Try disconnecting and reconnecting

**Issue**: Tool call doesn't trigger approval

**Solution**:

. Verify MCP tools are enabled in chat settings
. Check that the server is connected (green status)
. Restart the conversation

**Issue**: AI responds without using tools

**Solution**:

. Make your query more explicit about needing current/real-time data
. Check system prompt mentions available tools
. Verify the server connection is active

**Issue**: "Permission denied" when reading files

**Solution**:

. Check file permissions: `ls -la ~/documents/`
. Ensure the allowed directory path is correct
. Verify the MCP server has access to the path

**Issue**: AI can't find files

**Solution**:

. Verify files exist: `ls ~/documents/`
. Check the allowed directories configuration
. Use absolute paths in queries if needed

**Issue**: AI doesn't use tools for queries it should

**Solution**:

. Verify MCP servers are connected and tools enabled
. Check system prompt mentions available tools
. Be more explicit in your query about needing real data

**Issue**: Too many approval prompts for simple tasks

**Solution**:

. This is by design for safety
. Future versions may support pre-approved tool patterns
. Consider which tools truly need approval for your use case

**Issue**: AI gets confused with multiple tool results

**Solution**:

. Simplify queries to fewer tools at once
. Use clearer system prompts
. Try more capable models for complex reasoning
