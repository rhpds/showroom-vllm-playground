= Module 1: Getting Started with vLLM Playground
:source-highlighter: rouge
:toc: macro
:toclevels: 1

ACME Corporation is kicking off their AI customer support initiative, and the team has been tasked with evaluating vLLM as the foundation for their inference infrastructure. vLLM delivers 24x higher throughput compared to traditional inference methods, reducing inference costs by 85% and enabling ACME to process 10,000 customer support queries per minute on standard GPU infrastructure. vLLM Playground provides a modern web interface for managing and interacting with vLLM servers, making enterprise-grade AI inference accessible without complex DevOps overhead.

In this module, the presenter verifies the environment, deploys the first vLLM server using Podman, and demonstrates the chat interface. By the end, the audience will understand how vLLM's PagedAttention algorithm and efficient memory management enable ACME to handle 5x more concurrent customer conversations on the same hardware budget.

[[part-1]]
== Part 1 - Rapid Model Deployment

=== Know

_Customer challenge: ACME needs to evaluate AI inference options quickly. Manual model deployment takes days of DevOps effort, slowing experimentation and delaying AI adoption._

**Business Impact:**

* Development teams wait days for model deployment infrastructure to be provisioned
* Manual container configuration is error-prone, causing deployment failures
* Lack of standardized tooling creates inconsistency across environments
* Competition is shipping AI features faster while ACME is stuck on setup

**Value Proposition:**

vLLM Playground simplifies model deployment to a few clicks through a modern web interface backed by Podman containers. This eliminates DevOps bottlenecks and enables ACME's AI team to evaluate models in minutes rather than days.

=== Show

**What I say:**

"Before we can build ACME's AI-powered customer support, we need reliable model serving infrastructure. Let me show you how vLLM Playground makes this simple."

**What I do:**

. First, verify the environment is ready. Connect via SSH:
+
[source,bash,subs="attributes"]
----
{ssh_command}
----
+
When prompted, enter the password: `{ssh_password}`

. Verify vLLM Playground is installed:
+
[source,bash]
----
vllm-playground --help
----
+
Expected output:
+
----
usage: vllm-playground [-h] [--port PORT] [--host HOST] {pull,stop,status} ...

vLLM Playground - A modern web interface for vLLM

positional arguments:
  {pull,stop,status}
    pull              Pre-download container images
    stop              Stop running vLLM Playground instance
    status            Check status of vLLM Playground

optional arguments:
  -h, --help          show this help message and exit
  --port PORT         Port to run on (default: 7860)
  --host HOST         Host to bind to (default: 0.0.0.0)
----

. Check Podman is available:
+
[source,bash]
----
podman version
----
+
Podman version 4.0 or later should be displayed.

. Verify GPU availability:
+
[source,bash]
----
nvidia-smi
----
+
The NVIDIA GPU should be listed with driver and CUDA information.

. Check the current status of vLLM Playground:
+
[source,bash]
----
vllm-playground status
----
+
This shows whether any vLLM instance is currently running.

. In this demo environment, vLLM Playground is configured as a system daemon. Check its status using systemctl:
+
[source,bash]
----
sudo systemctl status vllm-playground
----
+
Expected output:
+
----
● vllm-playground.service - vLLM Playground Service
     Loaded: loaded (/etc/systemd/system/vllm-playground.service; enabled; preset: disabled)
     Active: active (running) since Sat 2026-01-10 03:00:18 UTC; 36min ago
 Invocation: 6185d5f065a34e3bbff96f54410c1943
   Main PID: 3975 (vllm-playground)
      Tasks: 4 (limit: 95955)
     Memory: 51.2M (peak: 68.5M)
        CPU: 11.483s
     CGroup: /system.slice/vllm-playground.service
             └─3975 /usr/bin/python3.12 /usr/local/bin/vllm-playground
----
+
The key indicators are:
+
* `Active: active (running)` - Service is running
* `enabled` - Service starts automatically on boot

**What they should notice:**

* Environment validation completes in seconds
* vLLM Playground is already running as a managed service
* GPU hardware is detected and ready

. Pre-download the GPU container image (this may take a few minutes):
+
[source,bash]
----
vllm-playground pull
----
+
This downloads the vLLM GPU container image (~10GB). Progress indicators show as the layers download.
+
NOTE: If in a CPU-only environment, use `vllm-playground pull --cpu` instead.
+
TIP: In this demo environment, the container image has been pre-pulled during setup. The command completes quickly as the image is already available locally.

. As verified earlier, vLLM Playground is already running as a daemon service. Open the Web UI:
+
link:{vllm_playground_url}[Open vLLM Playground^]
+
image::module-01-figure-01.png[vLLM Playground home page with server configuration panel on left showing TinyLlama model dropdown, run mode selection (Subprocess/Container), compute mode options (CPU/GPU), and green Start Server button; chat interface on right with message input area and response metrics panel,align="center",width=700,link=module-01-figure-01.png^,title="vLLM Playground Home Page"]

. In the vLLM Playground web interface, configure the first model:
+
.. Navigate to *Server Configuration* section
.. In the *Model* section, there are several options:
+
* *Dropdown list*: Select from pre-configured models like `TinyLlama 1.1B Chat (Fast, No token)`, ideal for quick testing
* *Custom model name*: Enter any HuggingFace model ID in the text field
* *Browse Community Recipes*: Explore community-contributed model configurations
* *HuggingFace Token*: Required only for gated models (Llama 3.1, Llama 3.2, etc.)
+
For this demo, select `TinyLlama 1.1B Chat` from the dropdown. It is fast and does not require authentication.
+
image::module-01-figure-02.png[Model selection dropdown menu displaying TinyLlama 1.1B Chat (Fast, No token) highlighted at top, with custom model name text field below and Browse Community Recipes button at bottom,align="center",width=300,link=module-01-figure-02.png^,title="Model Selection Options"]

.. In the *Run Mode* section, select how vLLM will run:
+
* *Subprocess*: Runs vLLM directly as a Python subprocess (good for development, requires vLLM preinstalled via pip install vllm)
* *Container*: Runs vLLM in an isolated Podman container (recommended for production)
+
Select *Container* mode. It provides better isolation and matches production deployments.
+
image::module-01-figure-03.png[Run Mode section with two radio buttons: Subprocess (for development, requires vLLM preinstalled) and Container (recommended, selected with blue highlight indicating isolated container execution),align="center",width=300,link=module-01-figure-03.png^,title="Run Mode Selection"]

.. In the *Compute Mode* section, select the hardware:
+
* *CPU*: For systems without GPU (slower inference)
* *GPU*: For NVIDIA GPU acceleration (recommended for performance)
+
Select *GPU* mode since the demo environment has GPU available.
+
image::module-01-figure-04.png[Compute Mode section with two radio buttons: CPU (for systems without GPU, slower inference) and GPU (selected with green highlight, recommended for high-performance inference on NVIDIA GPUs),align="center",width=300,link=module-01-figure-04.png^,title="Compute Mode Selection"]

.. Review the remaining default settings:
+
* *Port*: API endpoint port (default: 8000)
* *Tensor Parallel Size*: Number of GPUs for model parallelism
* *GPU Memory Utilization*: Fraction of GPU memory to use (default: 0.9)
* *Data Type*: Model precision (auto, float16, bfloat16)
* *Max Model Length*: Maximum sequence length
+
NOTE: These parameters are powered by vLLM's extensive configuration options, making it highly customizable for different hardware configurations and use cases. For this demo, the defaults work well.
+
As settings are adjusted, the *Command Preview* at the bottom dynamically generates the corresponding vLLM command that will run inside the container. This helps the audience understand exactly what parameters are being passed to vLLM.
+
image::module-01-figure-05.png[Command Preview panel displaying auto-generated vLLM container command with parameters: model TinyLlama-1.1B-Chat, GPU mode enabled, port 8000, tensor-parallel-size 1, showing full podman run command syntax with all selected configuration options,align="center",width=300,link=module-01-figure-05.png^,title="Command Preview"]

.. Click *Start Server*

. Wait for the model to load. Monitor progress in the interface or check container logs:
+
[source,bash]
----
sudo podman logs -f vllm-service
----
+
TIP: The `sudo` prefix is required because the vLLM container uses GPU resources. Access to GPU devices requires elevated privileges, so the container runs with root permissions.

**What they should notice:**

* The *Server Logs* panel shows `Application startup complete.`
* A green *"Server is ready to chat!"* toast notification appears
* The *Send* button in the chat interface turns green

image::module-01-figure-06.png[Server Logs panel showing Application startup complete message with green checkmark, plus green toast notification at top-right reading Server is ready to chat, and green-highlighted Send button in chat interface indicating the vLLM server is fully loaded and ready to accept requests,align="center",width=700,link=module-01-figure-06.png^,title="Server Ready Indicators"]

Verify the container is running via Podman:

[source,bash]
----
sudo podman ps
----

* Container `vllm-service` is running
* Status shows "healthy" or "Up"

**If asked:**

Q: "What if the container fails to start with 'out of memory'?"
A: "Check GPU memory with `nvidia-smi`. Try a smaller model or adjust GPU memory utilization to 0.8 for 80% usage."

Q: "Can we use models from sources other than HuggingFace?"
A: "Yes. vLLM supports any GGUF or HuggingFace-compatible model. You can also point to private model registries."

Q: "What about gated models like Llama?"
A: "For gated models, you need a HuggingFace token with access. Enter it in the HF_TOKEN field. Public models like TinyLlama and Qwen work without authentication."

[[part-2]]
== Part 2 - Modern Chat Interface

=== Know

_Customer challenge: ACME needs real-time customer interaction capabilities. Traditional batch processing introduces unacceptable delays in customer support scenarios._

**Business Impact:**

* Customers expect instant responses, similar to ChatGPT-like experiences
* Batch processing creates perceptible delays that reduce customer satisfaction
* Lack of system prompt control means AI responses are inconsistent and off-brand
* No visibility into inference performance makes capacity planning impossible

**Value Proposition:**

vLLM Playground provides a streaming chat interface with real-time performance metrics, system prompt customization, and temperature controls. This gives ACME's support team immediate visibility into AI behavior and performance.

=== Show

**What I say:**

"Now that we have the model running, let me show you what the customer interaction experience looks like. Notice how responses stream in real-time, just like ChatGPT."

**What I do:**

. In the vLLM Playground web UI, navigate to the *Chat* section.

. Point out the ChatGPT-style interface:
+
* Message input area at the bottom
* Conversation history in the main panel
* Model settings in the sidebar
+
image::module-01-figure-07.png[vLLM Playground chat interface with ChatGPT-style layout: conversation history panel in center showing AI responses, message input text area at bottom with blue Send button, and right sidebar displaying temperature slider (0.0-2.0), max tokens input field, and system prompt configuration button,align="center",width=700,link=module-01-figure-07.png^,title="Chat Interface"]

. Send the first message to test the model:
+
----
Hello! Can you introduce yourself and explain what you can help me with?
----
+
Point out the streaming response: text appears word by word as the model generates it.

. Test a customer support scenario for ACME:
+
----
I'm a customer support agent. A customer is asking about their order status. How should I professionally respond if their order is delayed by 2 days?
----
+
Notice how the model provides helpful guidance for the support scenario.

. Point out the *Response Metrics* panel on the right side of the chat interface. This built-in performance dashboard displays real-time metrics for each response:
+
* *Prompt Tokens*: Number of tokens in the input
* *Completion Tokens*: Number of tokens generated by the model
* *Total Tokens*: Combined input and output tokens
* *Time Taken*: Total response generation time
* *Tokens/sec*: Generation throughput rate
* *Avg Prompt Throughput*: Average processing speed for prompts
+
These metrics highlight the high throughput that vLLM delivers.
+
image::module-01-figure-08.png[Response Metrics panel displaying real-time performance statistics: Prompt Tokens: 24, Completion Tokens: 156, Total Tokens: 180, Time Taken: 2.3 seconds, Tokens/sec: 67.8 (generation speed), Avg Prompt Throughput: 10.4 tokens/sec, demonstrating vLLM high-throughput inference capabilities,align="center",width=300,link=module-01-figure-08.png^,title="Response Metrics"]

**What I say:**

"Now let me show you how system prompts let ACME customize the AI's persona for their customer support use case."

**What I do:**

. Explore the chat settings:
+
* *Temperature*: Controls randomness (lower = more deterministic)
* *Max Tokens*: Limits response length
* *System Prompt*: Sets the model's persona/behavior
+
Set a system prompt:
+
----
You are a helpful customer support assistant for ACME Corporation. Be professional, empathetic, and solution-oriented.
----
+
image::module-01-figure-09.png[System Prompt dialog box with large text area containing: You are a helpful customer support assistant for ACME Corporation. Be professional, empathetic, and solution-oriented. Below the text area are Templates dropdown button for preset prompts and Save button in blue, enabling customization of AI behavior for specific business scenarios,align="center",width=500,link=module-01-figure-09.png^,title="System Prompt Configuration"]
+
TIP: Click the *Templates* dropdown to access preset system prompt templates for common use cases like coding assistant, translator, or summarizer.
+
TIP: Notice the yellow indicator in the top-right corner of toolbar icons. This indicates the configuration has been changed from its default value.

. Send another message with the system prompt active:
+
----
A customer is frustrated because their product arrived damaged. What should I say?
----
+
Notice how the response now aligns with the customer support persona.

**What they should notice:**

* Messages send successfully and responses stream in real-time
* System prompt changes model behavior immediately
* Response metrics provide instant performance visibility
* The interface feels familiar, similar to ChatGPT

**If asked:**

Q: "Can we customize the chat interface for our brand?"
A: "vLLM Playground is open source, so the UI can be customized. For production, you would typically build a custom frontend that calls the vLLM OpenAI-compatible API."

Q: "Why are responses sometimes slow?"
A: "Check GPU utilization with `nvidia-smi`. Verify the model loaded to GPU, not CPU. For faster responses, consider a smaller model."

Q: "What models work best for customer support?"
A: "Models like Qwen2.5-3B-Instruct or Llama 3.1-8B provide good instruction-following for support scenarios. We will use Qwen in the next modules."

== Troubleshooting

**Issue**: Chat messages do not get responses

**Solution**:

. Verify server is running: Check the status indicator in the UI
. Check container logs: `podman logs vllm-service`
. Stop the Server and Start again

**Issue**: Responses are very slow

**Solution**:

. Check GPU utilization: `nvidia-smi`
. Verify model loaded to GPU (not CPU)
. Consider a smaller model for faster responses

**Issue**: Model gives unexpected or poor responses

**Solution**:

. Adjust temperature (try 0.7 for balanced responses)
. Add a clear system prompt to guide behavior
. Try rephrasing the question more specifically

== Module summary

**What was demonstrated:**

* Verified vLLM Playground installation and GPU availability
* Pulled container images and deployed a vLLM server in minutes
* Explored the modern chat interface with streaming responses
* Tested customer support scenarios relevant to ACME's use case
* Showed how system prompts customize AI behavior for specific business needs

**Key takeaways:**

* vLLM Playground simplifies LLM deployment through container management
* The chat interface provides a familiar experience similar to ChatGPT
* System prompts are essential for tailoring model behavior to specific use cases
* Streaming responses enable real-time interaction for better user experience
* Built-in metrics provide immediate performance visibility

**Business value for ACME:**

* Model deployment reduced from days to minutes
* Real-time customer interaction capabilities proven
* AI behavior customizable for specific support scenarios
* Performance visibility enables capacity planning

**Next module:**

Module 2 explores *Advanced Inferencing: Structured Outputs*, showing how to constrain model responses to specific formats using JSON Schema, Regex, and Grammar for predictable, parseable outputs.

== References

* link:https://github.com/micytao/vllm-playground[vLLM Playground GitHub Repository^] - Installation and configuration
* link:https://github.com/vllm-project/vllm[vLLM Project^] - High-performance inference engine
* link:https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0[TinyLlama Model^] - Lightweight model for testing
