= Module 1: Getting Started with vLLM Playground
:source-highlighter: rouge
:toc: macro
:toclevels: 1

ACME Corporation is kicking off their AI customer support initiative, and you've been tasked with evaluating vLLM as the foundation for their inference infrastructure. vLLM is the industry-leading high-throughtput and memory-efficient inference engine, and vLLM Playground provides a modern web interface for managing and interacting with vLLM servers.

In this module, you'll verify your environment, deploy your first vLLM server using Podman, and experience the chat interface firsthand.

== Learning objectives

By the end of this module, you'll be able to:

* Verify vLLM Playground installation and environment readiness
* Pull and deploy a vLLM server using Podman containers
* Navigate the vLLM Playground web interface
* Interact with an LLM through the chat UI with streaming responses

[[exercise-1]]
== Exercise 1: Verify your environment

ACME needs to ensure the AI infrastructure is properly configured before proceeding. Let's start by verifying that vLLM Playground and all dependencies are correctly installed on your lab environment.

You prepare to validate the environment to confirm ACME's infrastructure is ready for AI deployment.

. First, connect to your lab environment via SSH:
+
[source,bash,subs="attributes"]
----
{ssh_command}
----
+
When prompted, enter the password: `{ssh_password}`

. Verify vLLM Playground is installed:
+
[source,bash]
----
vllm-playground --help
----
+
Expected output:
+
----
usage: vllm-playground [-h] [--port PORT] [--host HOST] {pull,stop,status} ...

vLLM Playground - A modern web interface for vLLM

positional arguments:
  {pull,stop,status}
    pull              Pre-download container images
    stop              Stop running vLLM Playground instance
    status            Check status of vLLM Playground

optional arguments:
  -h, --help          show this help message and exit
  --port PORT         Port to run on (default: 7860)
  --host HOST         Host to bind to (default: 0.0.0.0)
----

. Check Podman is available:
+
[source,bash]
----
podman version
----
+
You should see Podman version 5.0 or later.

. Verify GPU availability:
+
[source,bash]
----
nvidia-smi
----
+
You should see your NVIDIA GPU listed with driver and CUDA information.

. Check the current status of vLLM Playground:
+
[source,bash]
----
vllm-playground status
----
+
This shows whether any vLLM instance is currently running.

. In this lab environment, vLLM Playground is configured as a system daemon. You can also check its status using systemctl:
+
[source,bash]
----
sudo systemctl status vllm-playground
----
+
Expected output:
+
----
● vllm-playground.service - vLLM Playground Service
     Loaded: loaded (/etc/systemd/system/vllm-playground.service; enabled; preset: disabled)
     Active: active (running) since Sat 2026-01-10 03:00:18 UTC; 36min ago
 Invocation: 6185d5f065a34e3bbff96f54410c1943
   Main PID: 3975 (vllm-playground)
      Tasks: 4 (limit: 95955)
     Memory: 51.2M (peak: 68.5M)
        CPU: 11.483s
     CGroup: /system.slice/vllm-playground.service
             └─3975 /usr/bin/python3.12 /usr/local/bin/vllm-playground
----
+
The key indicators are:
+
* `Active: active (running)` - Service is running
* `enabled` - Service starts automatically on boot

=== Verify

Confirm all checks passed:

✓ `vllm-playground --help` displays usage information

✓ `podman version` shows version 4.0+

✓ `nvidia-smi` shows GPU information

✓ Environment is ready for vLLM deployment

[[exercise-2]]
== Exercise 2: Deploy your first vLLM server

With the environment validated, ACME is ready to deploy their first LLM. You'll pull the necessary container images and start a vLLM server with a capable model for customer support scenarios.

=== Prerequisites

* Exercise 1 completed successfully
* GPU detected and available

=== Steps

. Pre-download the GPU container image (this may take a few minutes):
+
[source,bash]
----
vllm-playground pull
----
+
This downloads the vLLM GPU container image (~10GB). You'll see progress indicators as the layers download.
+
NOTE: If you're in a CPU-only environment, use `vllm-playground pull --cpu` instead.
+
TIP: In this lab environment, the container image has been pre-pulled during setup. The command will complete quickly as the image is already available locally.

. As verified in Exercise 1, vLLM Playground is already running as a daemon service. Access the Web UI:
+
link:http://{hostname}:7860[Open vLLM Playground^]
+
image::module-01-figure-01.png[vLLM Playground home page showing the main interface,align="center",width=700,link=module-01-figure-01.png^,title="vLLM Playground Home Page"]

. In the vLLM Playground web interface, configure your first model:
+
.. Navigate to *Server Configuration* section
.. In the *Model* section, you have several options:
+
* *Dropdown list*: Select from pre-configured models like `TinyLlama 1.1B Chat (Fast, No token)` - ideal for quick testing
* *Custom model name*: Enter any HuggingFace model ID in the text field
* *Browse Community Recipes*: Explore community-contributed model configurations
* *HuggingFace Token*: Required only for gated models (Llama 3.1, Llama 3.2, etc.)
+
For this exercise, select `TinyLlama 1.1B Chat` from the dropdown - it's fast and doesn't require authentication.
+
image::module-01-figure-02.png[Model selection showing dropdown and custom model options,align="center",width=300,link=module-01-figure-02.png^,title="Model Selection Options"]

.. In the *Run Mode* section, select how vLLM will run:
+
* *Subprocess*: Runs vLLM directly as a Python subprocess (good for development, requires vLLM preinstalled via pip install vllm)
* *Container*: Runs vLLM in an isolated Podman container (recommended for production)
+
For this exercise, select *Container* mode - it provides better isolation and matches production deployments.
+
image::module-01-figure-03.png[Run Mode selection showing Subprocess and Container options,align="center",width=300,link=module-01-figure-03.png^,title="Run Mode Selection"]

.. In the *Compute Mode* section, select your hardware:
+
* *CPU*: For systems without GPU (slower inference)
* *GPU*: For NVIDIA GPU acceleration (recommended for performance)
+
Select *GPU* mode since your lab environment has GPU available.
+
image::module-01-figure-04.png[Compute Mode selection showing CPU and GPU options,align="center",width=300,link=module-01-figure-04.png^,title="Compute Mode Selection"]

.. Review the remaining default settings:
+
* *Port*: API endpoint port (default: 8000)
* *Tensor Parallel Size*: Number of GPUs for model parallelism
* *GPU Memory Utilization*: Fraction of GPU memory to use (default: 0.9)
* *Data Type*: Model precision (auto, float16, bfloat16)
* *Max Model Length*: Maximum sequence length
+
NOTE: These parameters are powered by vLLM's extensive configuration options, making it highly customizable for different hardware configurations and use cases. For this exercise, the defaults work well.
+
As you adjust these settings, the *Command Preview* at the bottom dynamically generates the corresponding vLLM command that will run inside the container. This helps you understand exactly what parameters are being passed to vLLM.
+
image::module-01-figure-05.png[Command Preview showing the generated vLLM command based on selected parameters,align="center",width=300,link=module-01-figure-05.png^,title="Command Preview"]

.. Click *Start Server*

. Wait for the model to load. You can monitor progress in the interface or check container logs:
+
[source,bash]
----
sudo podman logs -f vllm-service
----
+
TIP: The `sudo` prefix is required because the vLLM container uses GPU resources. Access to GPU devices requires elevated privileges, so the container runs with root permissions.

=== Verify

Look for these indicators that the server is ready:

* The *Server Logs* panel shows `Application startup complete.`
* A green *"Server is ready to chat!"* toast notification appears
* The *Send* button in the chat interface turns green

image::module-01-figure-06.png[Server Logs showing Application startup complete and Server is ready to chat notification,align="center",width=700,link=module-01-figure-06.png^,title="Server Ready Indicators"]

You can also verify the container is running via Podman:

[source,bash]
----
sudo podman ps
----

✓ Container `vllm-service` is running
✓ Status shows "healthy" or "Up"

=== Troubleshooting

**Issue**: Container fails to start with "out of memory"

**Solution**: 

. Check GPU memory: `nvidia-smi`
. Try a smaller model or adjust GPU memory utilization in settings
. Use `--gpu-memory-utilization 0.8` for 80% GPU memory usage

**Issue**: "Model not found" error

**Solution**:

. Verify model ID is correct (check HuggingFace)
. For gated models (Llama, Gemma), ensure you have access and set `HF_TOKEN`
. Try a public model like `TinyLlama/TinyLlama-1.1B-Chat-v1.0`

**Issue**: Port 7860 already in use

**Solution**:

[source,bash]
----
vllm-playground stop
vllm-playground --port 8080
----

[[exercise-3]]
== Exercise 3: Interact with your LLM

Now that the vLLM server is running, you'll explore the chat interface and have your first conversation with the model. This simulates how ACME's customer support system would interact with users.

=== Steps

. In the vLLM Playground web UI, navigate to the *Chat* section.

. You'll see a ChatGPT-style interface with:
+
* Message input area at the bottom
* Conversation history in the main panel
* Model settings in the sidebar
+
image::module-01-figure-07.png[vLLM Playground chat interface showing conversation area with message input and Send button,align="center",width=700,link=module-01-figure-07.png^,title="Chat Interface"]

. Send your first message to test the model:
+
----
Hello! Can you introduce yourself and explain what you can help me with?
----
+
Observe the streaming response - text appears word by word as the model generates it.

. Test a customer support scenario for ACME:
+
----
I'm a customer support agent. A customer is asking about their order status. How should I professionally respond if their order is delayed by 2 days?
----
+
Notice how the model provides helpful guidance for the support scenario.

. Observe the *Response Metrics* panel on the right side of the chat interface. This built-in performance dashboard displays real-time metrics for each response:
+
* *Prompt Tokens*: Number of tokens in your input
* *Completion Tokens*: Number of tokens generated by the model
* *Total Tokens*: Combined input and output tokens
* *Time Taken*: Total response generation time
* *Tokens/sec*: Generation throughput rate
* *Avg Prompt Throughput*: Average processing speed for prompts
+
These metrics highlight the high throughput that vLLM delivers.
+
image::module-01-figure-08.png[Response Metrics panel showing throughput and token statistics,align="center",width=300,link=module-01-figure-08.png^,title="Response Metrics"]

. Explore the chat settings:
+
* *Temperature*: Controls randomness (lower = more deterministic)
* *Max Tokens*: Limits response length
* *System Prompt*: Sets the model's persona/behavior
+
Try setting a system prompt:
+
----
You are a helpful customer support assistant for ACME Corporation. Be professional, empathetic, and solution-oriented.
----
+
image::module-01-figure-09.png[System Prompt dialog showing the ACME customer support persona configuration,align="center",width=500,link=module-01-figure-09.png^,title="System Prompt Configuration"]
+
TIP: Click the *Templates* dropdown to access preset system prompt templates for common use cases like coding assistant, translator, or summarizer.
+
TIP: Notice the yellow indicator in the top-right corner of toolbar icons - this indicates the configuration has been changed from its default value.

. Send another message with the system prompt active:
+
----
A customer is frustrated because their product arrived damaged. What should I say?
----
+
Notice how the response now aligns with the customer support persona.

=== Verify

Confirm you can interact with the model:

✓ Messages send successfully

✓ Responses stream in real-time (word by word)

✓ System prompt affects model behavior

✓ Different temperature settings change response style

== Troubleshooting

**Issue**: Chat messages don't get responses

**Solution**:

. Verify server is running: Check the status indicator in the UI
. Check container logs: `podman logs vllm-service`
. Stop the Server and Start again

**Issue**: Responses are very slow

**Solution**:

. Check GPU utilization: `nvidia-smi`
. Verify model loaded to GPU (not CPU)
. Consider a smaller model for faster responses

**Issue**: Model gives unexpected or poor responses

**Solution**:

. Adjust temperature (try 0.7 for balanced responses)
. Add a clear system prompt to guide behavior
. Try rephrasing your question more specifically

== Learning outcomes

By completing this module, you should now understand:

* ✓ How vLLM Playground provides a web interface for managing vLLM servers
* ✓ The container-based architecture that enables easy deployment via Podman
* ✓ How streaming responses work for real-time user interaction
* ✓ The role of system prompts and temperature in controlling model behavior
* ✓ Basic troubleshooting for vLLM server issues

== Module summary

You've successfully completed the Getting Started module for vLLM Playground.

**What you accomplished:**

* Verified vLLM Playground installation and GPU availability
* Pulled container images and deployed a vLLM server
* Explored the modern chat interface with streaming responses
* Tested customer support scenarios relevant to ACME's use case

**Key takeaways:**

* vLLM Playground simplifies LLM deployment through container management
* The chat interface provides a familiar experience similar to ChatGPT
* System prompts are essential for tailoring model behavior to specific use cases
* Streaming responses enable real-time interaction for better user experience

**Next steps:**

Module 2 will explore *Advanced Inferencing: Structured Outputs* - learning how to constrain model responses to specific formats using JSON Schema, Regex, and Grammar for predictable, parseable outputs.

== References

* link:https://github.com/micytao/vllm-playground[vLLM Playground GitHub Repository^] - Installation and configuration
* link:https://github.com/vllm-project/vllm[vLLM Project^] - High-performance inference engine
* link:https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0[TinyLlama Model^] - Lightweight model for testing
