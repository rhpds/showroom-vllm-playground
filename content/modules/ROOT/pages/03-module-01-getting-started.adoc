= Module 1: Getting started with vLLM Playground
:source-highlighter: rouge
:toc: macro
:toclevels: 1

ACME Corporation is kicking off their AI customer support initiative, and this demonstration shows how they evaluated vLLM as the foundation for their inference infrastructure. vLLM is the industry-leading high-throughput and memory-efficient inference engine, and vLLM Playground provides a modern web interface for managing and interacting with vLLM servers.

In this module, you'll see how ACME deploys their first vLLM server using Podman and experiences the chat interface for customer support scenarios.

**Presenter note**: This module forms the core of the 15-20 minute executive brief. Focus on business value and rapid deployment capabilities.

== Part 1 — Rapid model deployment with containers

=== Know

_ACME Corporation faces a critical challenge: their current AI deployment process takes days or weeks of manual setup, preventing them from iterating quickly on their customer support AI initiative._

**Business challenge**: Manual model deployment

* Traditional AI deployment requires ML engineers to provision infrastructure, configure dependencies, and manually deploy models
* This process takes 3-5 days per model deployment, blocking rapid experimentation
* DevOps teams become bottlenecks as they handle every model update
* Inconsistent deployment procedures lead to errors and rework

**Current state pain points**:

* Development teams wait days for AI infrastructure provisioning
* Manual deployment steps are error-prone and undocumented
* Scaling to multiple models multiplies the operational overhead
* No self-service capability for data scientists to test models

**Desired state transformation**:

* Deploy AI models in minutes, not days - eliminating infrastructure wait time
* Container-based deployment ensures consistency and repeatability
* Self-service model deployment empowers data scientists
* Production-ready architecture from day one with Podman containers

**Business value proposition**:

vLLM Playground eliminates weeks of manual deployment effort by leveraging container technology. What traditionally takes 3-5 days of infrastructure provisioning now happens in under 10 minutes through automated container deployment.

**Quantified business impact**:

* **Time savings**: 3-5 days → 10 minutes model deployment (99% faster)
* **Cost reduction**: Eliminate 20-40 hours of engineering time per model deployment
* **Faster iteration**: Data scientists can test and deploy models independently
* **Production confidence**: Container-based deployment matches enterprise standards

**Stakeholder impact**:

* **VP of Engineering**: Reduce infrastructure team bottlenecks and accelerate AI initiatives
* **Data Science Team**: Gain self-service capability to test and deploy models without waiting for DevOps
* **Platform Team**: Standardize AI deployment with container best practices

=== Show

**Presenter guidance**: This demonstration shows how ACME moves from days-long manual deployment to minutes-long container-based deployment. Emphasize the simplicity and speed throughout.

**What to demonstrate**:

. **Connect to demo environment** (30 seconds):
+
[source,bash,subs="attributes"]
----
{ssh_command}
----
+
When prompted, password: `{ssh_password}`
+
**Business context**: "We're connecting to ACME's GPU-enabled inference host where we'll deploy vLLM in containers."

. **Verify environment readiness** (1 minute):
+
[source,bash]
----
vllm-playground --help
----
+
**What they'll see**: vLLM Playground CLI usage information
+
**Business talking point**: "vLLM Playground provides a CLI and web UI for managing vLLM servers. No complex infrastructure setup required."
+
Check Podman:
+
[source,bash]
----
podman version
----
+
**What they'll see**: Podman version 5.0+
+
**Business talking point**: "Podman provides Docker-compatible container management without requiring a daemon, making it ideal for enterprise Linux environments."
+
Verify GPU (optional, depending on audience):
+
[source,bash]
----
nvidia-smi
----
+
**What they'll see**: NVIDIA GPU information with CUDA
+
**Business talking point**: "vLLM leverages GPU acceleration for high-performance inference, delivering throughput 10-100x faster than CPU-only solutions."

. **Access vLLM Playground web interface** (1 minute):
+
link:http://{hostname}:7860[Open vLLM Playground^]
+
**What they'll see**: vLLM Playground home page
+
**Business talking point**: "ACME's team uses this modern web interface to deploy and manage multiple LLM servers without writing deployment scripts."
+
image::module-01-figure-01.png[vLLM Playground home page showing the main interface,width=700,title="vLLM Playground Home Page"]

. **Configure and deploy vLLM server** (3-4 minutes):
+
**Navigate to Server Configuration section**
+
**In Model section**:
+
* Show the dropdown list with pre-configured models
* Select `TinyLlama 1.1B Chat (Fast, No token)` for this demo
* **Business talking point**: "For production, ACME would use larger models like Llama 3.1 or Mistral, but TinyLlama deploys quickly for demonstrations."
+
image::module-01-figure-02.png[Model selection showing dropdown and custom model options,width=300,title="Model Selection Options"]
+
**In Run Mode section**:
+
* Show Subprocess vs Container options
* Select *Container* mode
* **Business talking point**: "Container mode provides isolation and matches production deployment patterns. This same approach scales from laptop testing to enterprise Kubernetes clusters."
+
image::module-01-figure-03.png[Run Mode selection showing Subprocess and Container options,width=300,title="Run Mode Selection"]
+
**In Compute Mode section**:
+
* Show CPU vs GPU options
* Select *GPU* mode
* **Business talking point**: "GPU acceleration delivers the throughput needed for production customer support workloads. ACME will serve hundreds of concurrent requests with this setup."
+
image::module-01-figure-04.png[Compute Mode selection showing CPU and GPU options,width=300,title="Compute Mode Selection"]
+
**Review configuration parameters**:
+
* Port: 8000 (API endpoint)
* Tensor Parallel Size, GPU Memory Utilization, Data Type, Max Model Length
* **Business talking point**: "These parameters are powered by vLLM's extensive configuration options, making it highly customizable for different hardware configurations and use cases."
+
**Point out Command Preview at bottom**:
+
image::module-01-figure-05.png[Command Preview showing the generated vLLM command based on selected parameters,width=300,title="Command Preview"]
+
**Business talking point**: "The command preview shows exactly what will run inside the container. This transparency helps DevOps teams understand and customize deployments."
+
**Click Start Server**

. **Monitor deployment progress** (1-2 minutes):
+
**What they'll see**: Server logs streaming in the UI
+
**Business talking point**: "The model is loading into GPU memory. In production, ACME would pre-load models to eliminate this startup time."
+
**Optional**: Show container logs via CLI:
+
[source,bash]
----
sudo podman logs -f vllm-service
----
+
**Explain why sudo**: "The vLLM container uses GPU resources which require elevated privileges for device access."

. **Confirm deployment success** (30 seconds):
+
**What they'll see**:
+
* Server Logs panel shows `Application startup complete.`
* Green toast notification: "Server is ready to chat!"
* Send button in chat interface turns green
+
image::module-01-figure-06.png[Server Logs showing Application startup complete and Server is ready to chat notification,width=700,title="Server Ready Indicators"]
+
**Business value callout**: "What traditionally took ACME's team 3-5 days of manual infrastructure setup just completed in under 10 minutes through automated container deployment. This eliminates deployment bottlenecks and enables rapid AI iteration."
+
**Verify via CLI** (optional):
+
[source,bash]
----
sudo podman ps
----
+
**What they'll see**: `vllm-service` container running with "healthy" status

**Troubleshooting scenarios to be ready for**:

**Q: "What if the container fails to start with out of memory?"**
→ A: Check GPU memory with `nvidia-smi` and adjust GPU memory utilization parameter. For smaller GPUs, use a smaller model or reduce the utilization fraction.

**Q: "How does this scale to production workloads?"**
→ A: vLLM supports horizontal scaling across multiple GPUs (tensor parallelism) and multiple servers. ACME can deploy this same container image to Kubernetes/OpenShift for enterprise-scale deployment.

**Q: "What about model security and gated access?"**
→ A: For gated models like Llama 3.1, vLLM Playground supports HuggingFace tokens. ACME can also deploy models from private registries or local storage.

== Part 2 — Modern chat interface for customer support

=== Know

_Once the vLLM server is deployed, ACME needs to validate that the AI can handle real customer support scenarios with a familiar, intuitive interface._

**Business challenge**: User experience expectations

* Modern users expect ChatGPT-style conversational interfaces with streaming responses
* Legacy chatbots provide rigid, slow responses that frustrate users
* Customer support agents need to test AI responses before deploying to production
* IT teams need visibility into AI performance metrics for capacity planning

**Current state limitations**:

* Batch-style AI responses force users to wait for complete answers
* No real-time feedback during response generation
* Lack of performance visibility makes capacity planning guesswork
* Difficult to customize AI persona for brand voice

**Desired state transformation**:

* Streaming responses provide real-time interaction like ChatGPT
* System prompts enable brand-specific AI personas
* Built-in performance metrics inform capacity planning
* Familiar interface reduces training time for support agents

**Business value proposition**:

vLLM Playground's chat interface delivers the modern conversational experience customers expect while providing the performance visibility and customization controls enterprises need.

**Stakeholder impact**:

* **Customer Support Managers**: Validate AI responses before deploying to customers, ensuring quality
* **UX Teams**: Deliver modern streaming chat experience that meets user expectations
* **Operations Teams**: Access real-time performance metrics for capacity planning and SLA monitoring

=== Show

**Presenter guidance**: Demonstrate how ACME's support team uses the chat interface to test customer scenarios. Make the interaction natural and conversational.

**What to demonstrate**:

. **Navigate to Chat section** (15 seconds):
+
**What they'll see**: ChatGPT-style interface with message input, conversation history, and settings sidebar
+
**Business talking point**: "This familiar interface means minimal training for ACME's support agents. They already know how to use chat interfaces."
+
image::module-01-figure-07.png[vLLM Playground chat interface showing conversation area with message input and Send button,width=700,title="Chat Interface"]

. **Send first test message** (1 minute):
+
Type and send:
+
----
Hello! Can you introduce yourself and explain what you can help me with?
----
+
**What they'll see**: Streaming response - text appears word by word
+
**Business talking point**: "Notice the streaming response. This real-time generation creates a more engaging user experience compared to batch-style 'wait for the full answer' approaches."

. **Test customer support scenario** (2 minutes):
+
Send a realistic support query:
+
----
I'm a customer support agent. A customer is asking about their order status. How should I professionally respond if their order is delayed by 2 days?
----
+
**What they'll see**: Helpful guidance for the support scenario
+
**Business talking point**: "This demonstrates how ACME's support agents can use AI as a real-time assistant to craft better customer responses. The AI suggests professional, empathetic language."

. **Demonstrate performance metrics** (1 minute):
+
**Point to Response Metrics panel on right side**:
+
* Prompt Tokens: Input token count
* Completion Tokens: Generated token count
* Total Tokens: Combined count
* Time Taken: Response generation time
* Tokens/sec: Throughput rate
* Avg Prompt Throughput: Processing speed
+
image::module-01-figure-08.png[Response Metrics panel showing throughput and token statistics,width=300,title="Response Metrics"]
+
**Business talking point**: "These real-time metrics help ACME's operations team understand throughput capacity. For production planning, they can calculate: 'At 50 tokens/second throughput, we can serve X concurrent support conversations.'"

. **Configure system prompt for brand persona** (2-3 minutes):
+
**Open system prompt settings**
+
Configure ACME's brand voice:
+
----
You are a helpful customer support assistant for ACME Corporation. Be professional, empathetic, and solution-oriented. Always prioritize customer satisfaction while following company policies.
----
+
image::module-01-figure-09.png[System Prompt dialog showing the ACME customer support persona configuration,width=500,title="System Prompt Configuration"]
+
**Business talking point**: "System prompts let ACME define their AI's personality and behavior. This ensures consistent brand voice across all AI-generated responses."
+
**Show Templates dropdown** (optional):
+
**Business talking point**: "vLLM Playground includes preset templates for common use cases. For production, ACME would create custom templates for different support scenarios."
+
**Point out yellow indicator on toolbar** (UI detail):
+
**Business talking point**: "The yellow indicator shows configuration changes from defaults, helping operators track customizations."

. **Test with system prompt active** (1 minute):
+
Send another support scenario:
+
----
A customer is frustrated because their product arrived damaged. What should I say?
----
+
**What they'll see**: Response now aligns with ACME's customer support persona
+
**Business value callout**: "Notice how the AI now reflects ACME's brand voice - professional, empathetic, and solution-oriented. This consistency is critical for maintaining brand reputation in customer interactions."

. **Demonstrate temperature control** (optional, if time permits):
+
**Adjust temperature slider**:
+
* Show Temperature = 0.2 (deterministic)
* vs Temperature = 0.9 (creative)
+
**Business talking point**: "Lower temperatures provide consistent, predictable responses for structured support scenarios. Higher temperatures enable more creative problem-solving when needed."

**Business value summary**:

"What you've seen is how ACME deploys production-ready AI in minutes using container technology, then validates it with a modern chat interface. This eliminates 3-5 days of manual deployment work and provides the streaming chat experience customers expect."

**Connection to next modules** (if doing longer demo):

"In Module 2, we'll show how ACME ensures consistent, structured AI outputs for integration with their CRM and ticketing systems. Module 3 demonstrates tool calling - enabling AI to retrieve real customer data and execute support actions."

== Demonstration outcomes

By seeing this demonstration, audiences should understand:

* ✓ How vLLM Playground eliminates manual AI deployment overhead through container automation
* ✓ The business impact of reducing deployment time from days to minutes
* ✓ How streaming responses deliver modern user experiences
* ✓ The role of system prompts in controlling AI persona for brand consistency
* ✓ Real-time performance metrics that enable capacity planning

== Module summary

**What ACME accomplished**:

* Deployed vLLM server in under 10 minutes using containerized architecture
* Validated chat interface with streaming responses for customer support scenarios
* Configured system prompt to align AI responses with brand voice
* Accessed performance metrics for production capacity planning

**Key takeaways for technical audiences**:

* vLLM Playground simplifies LLM deployment through container management and web UI
* Container-based architecture enables consistent deployments from dev to production
* System prompts are essential for tailoring model behavior to specific use cases
* Built-in performance metrics support data-driven capacity planning

**Business value delivered**:

* **99% faster deployment**: 3-5 days → 10 minutes
* **Self-service enablement**: Data scientists deploy models without DevOps bottlenecks
* **Modern UX**: Streaming chat meets customer expectations
* **Operational visibility**: Real-time metrics inform scaling decisions

**Next module preview**:

Module 2 explores *Structured Outputs* - learning how ACME constrains model responses to specific formats using JSON Schema, Regex, and Grammar for predictable, parseable outputs that integrate with CRM and ticketing systems.
