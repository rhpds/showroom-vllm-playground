= Module 5: Performance testing for production readiness
:source-highlighter: rouge
:toc: macro
:toclevels: 1

Throughout this demonstration, ACME deployed vLLM servers, configured structured outputs, enabled tool calling, and explored MCP integration. Now they face the final question before production deployment: **Can this infrastructure handle real-world load?**

Before launching AI-powered customer support, ACME needs quantified performance data. GuideLLM is a benchmarking tool designed specifically for LLM inference servers, providing insights into throughput, latency, and resource utilization that inform capacity planning and SLA commitments.

**Presenter note**: This module is optional for 60-minute deep dives. It demonstrates performance validation that moves projects from proof-of-concept to production deployment.

== Part 1 — Performance metrics for production planning

=== Know

_ACME's VP of Engineering has approved the AI customer support project, but operations teams need hard numbers: How many concurrent customers can we serve? What's the response latency? Can we commit to SLAs? How much GPU infrastructure do we need?_

**Business challenge**: Production capacity planning

* Proof-of-concept demos don't provide capacity planning data
* Operations teams can't deploy without SLA commitments
* Infrastructure costs depend on throughput requirements
* Performance under load is unknown until tested

**Current state without benchmarking**:

* **Capacity guesswork**: No data on how many concurrent users the system supports
* **No SLA basis**: Can't commit to response time SLAs without measurements
* **Cost uncertainty**: Don't know if current infrastructure is over/under-provisioned
* **Risk of failure**: Production load might overwhelm untested infrastructure

**Desired state with performance testing**:

* Quantified throughput metrics (requests per second)
* Measured latency for capacity planning (time to first token, end-to-end)
* GPU utilization data for infrastructure optimization
* Confidence in SLA commitments based on real measurements

**Business value proposition**:

Performance benchmarking transforms proof-of-concept into production-ready deployment. ACME gains quantified data for capacity planning, SLA commitments, and infrastructure budgeting.

**Quantified business impact**:

* **SLA confidence**: Commit to response time SLAs based on measured performance
* **Capacity planning**: Calculate exact infrastructure needed for target user load
* **Cost optimization**: Right-size infrastructure based on actual utilization data
* **Risk mitigation**: Identify performance bottlenecks before production launch

**Stakeholder impact**:

* **Operations Teams**: Data-driven capacity planning and infrastructure sizing
* **Finance**: Accurate infrastructure cost forecasting based on measured requirements
* **Executive Leadership**: Confidence in production readiness with quantified metrics
* **Customer Success**: SLA commitments backed by performance data

=== Show

**Presenter guidance**: Demonstrate performance benchmarking using GuideLLM. Show how ACME collects the quantified data needed for production deployment decisions.

**What to demonstrate**:

. **Understanding performance metrics** (2-3 minutes):
+
**Present the metrics table**:
+
[cols="1,2,2"]
|===
|Metric |Description |Business Impact

|**Throughput**
|Requests processed per second
|How many customers can be served simultaneously

|**TTFT (Time to First Token)**
|How quickly the response starts
|User-perceived responsiveness

|**End-to-End Latency**
|Total time for complete response
|Total customer wait time

|**Tokens/second**
|Rate of token generation
|How fast responses stream to users

|**GPU Utilization**
|Percentage of GPU compute used
|Infrastructure efficiency and cost

|**Memory Usage**
|GPU memory consumption
|Capacity for concurrent requests
|===
+
**Business talking point**: "These metrics directly inform ACME's SLA commitments. If benchmarks show 2-second time-to-first-token and 50 requests/second throughput, operations can commit to '99% of customers receive first response within 3 seconds under normal load.'"

. **Access vLLM Playground performance panel** (1 minute):
+
Open browser to: http://{hostname}:7860
+
**Navigate to Performance/Benchmarking section** (if available in UI)
+
**Business context**: "vLLM Playground integrates GuideLLM for performance testing. ACME can benchmark directly through the UI without separate tooling."

. **Configure benchmark parameters** (2 minutes):
+
**Show benchmark configuration**:
+
* **Model**: Currently running model (e.g., Qwen2.5-3B)
* **Request rate**: Number of concurrent requests (e.g., 10, 20, 50)
* **Duration**: How long to run the test (e.g., 60 seconds)
* **Request pattern**: Realistic customer support queries
+
**Business talking point**: "ACME sets request rate based on expected customer volume. If they expect 500 concurrent support chats during peak hours, they'd test at that load level."

. **Run performance benchmark** (3-4 minutes):
+
**Click Run Benchmark**
+
**What they'll see**:
+
* Real-time throughput graph showing requests/second
* Latency measurements (TTFT and end-to-end)
* Token generation rate
* GPU utilization metrics
+
**Business talking point**: "The benchmark simulates real customer load. Watch the throughput line - this shows how many concurrent conversations the infrastructure can handle."

. **Analyze results** (3-4 minutes):
+
**Point to key metrics in results**:
+
* **Throughput**: "50 requests/second sustained throughput"
* **TTFT**: "Average 1.8 seconds to first token"
* **End-to-end latency**: "Average 12 seconds for complete responses"
* **Tokens/sec**: "45 tokens/second generation rate"
* **GPU utilization**: "78% average GPU utilization"
+
**Business value callout**: "These numbers tell ACME's story. With current infrastructure: 50 concurrent customer conversations, sub-2-second initial response, full answers in 12 seconds. Operations can now commit to SLAs and calculate infrastructure scaling: For 500 concurrent users, we need 10x this GPU capacity."

. **Interpret for capacity planning** (2-3 minutes):
+
**Business talking point - capacity planning**:
+
"Based on these results, ACME can calculate:
+
* **Current capacity**: 50 concurrent conversations on single GPU
* **Peak hour target**: 500 concurrent conversations needed
* **Infrastructure requirement**: 10 GPUs for peak load + 20% buffer = 12 GPUs
* **Cost estimate**: 12 x GPU hourly cost x hours/month = monthly infrastructure budget
* **SLA commitment**: 'First response within 3 seconds, 95th percentile' (measured at 1.8s average, 2.5s p95)"

. **Show optimization opportunity** (1-2 minutes):
+
**Point to GPU utilization** (78%):
+
**Business talking point**: "78% utilization suggests room for optimization. ACME could increase GPU memory utilization parameter to serve more concurrent requests per GPU, reducing infrastructure costs by 10-15%."

**Business value summary**:

"Performance benchmarking transforms guesswork into data-driven decisions. ACME now has quantified metrics for SLA commitments, exact infrastructure requirements for capacity planning, and cost forecasts based on measured utilization. This is how proof-of-concept becomes production deployment."

**Troubleshooting scenarios to be ready for**:

**Q: "How do we account for traffic spikes and seasonality?"**
→ A: Benchmark at peak expected load plus buffer. ACME might test at 150% of expected peak to ensure infrastructure handles Black Friday traffic spikes.

**Q: "What if our benchmark results don't meet SLA targets?"**
→ A: Optimize model size (smaller models faster inference), increase GPU resources, or adjust SLA targets based on reality. Better to know before production than after launch.

**Q: "How often should we re-benchmark?"**
→ A: After model changes, vLLM version upgrades, or infrastructure modifications. Treat it like load testing in traditional software development.

**Q: "Can we benchmark different models for comparison?"**
→ A: Yes. ACME might benchmark TinyLlama, Qwen2.5-3B, and Qwen2.5-7B to find the optimal balance of capability vs performance for their use case.

== Part 2 — Advanced performance optimization (Optional - if time permits)

=== Know

_Production deployments often require performance tuning: Adjusting GPU memory utilization, tensor parallelism for multi-GPU setups, and request batching for throughput optimization._

**Advanced optimization techniques**:

* **GPU memory utilization**: Increasing from 0.9 to 0.95 serves more concurrent requests
* **Tensor parallelism**: Distributing large models across multiple GPUs
* **Request batching**: Optimizing batch sizes for throughput vs latency tradeoffs
* **Model quantization**: Using smaller precision (FP16, INT8) for faster inference

=== Show

**Presenter guidance** (only if extra time): Briefly mention optimization techniques without deep demonstration.

**Key points to cover**:

* **Memory optimization**: "Increasing GPU memory utilization from 0.9 to 0.95 might serve 5-10% more concurrent requests."
* **Multi-GPU scaling**: "For large models, ACME could use tensor parallelism to split the model across 2-4 GPUs, trading cost for capability."
* **Batch tuning**: "Adjusting request batch sizes optimizes the throughput vs latency tradeoff based on ACME's priorities."

**Business value summary for optimization**:

"Performance optimization reduces infrastructure costs by 10-30% while maintaining SLA targets. ACME can serve more customers with existing hardware through data-driven tuning."

== Demonstration outcomes

By seeing this demonstration, audiences should understand:

* ✓ How performance benchmarking informs production deployment decisions
* ✓ The business value of quantified capacity planning data
* ✓ Key metrics that drive SLA commitments and infrastructure sizing
* ✓ Real-world application to moving from POC to production
* ✓ How to calculate infrastructure costs based on measured performance

== Module summary

**What ACME accomplished**:

* Benchmarked vLLM server with GuideLLM under realistic load
* Measured throughput, latency, and resource utilization metrics
* Calculated infrastructure requirements for target capacity
* (Optional) Explored performance optimization techniques

**Key takeaways for technical audiences**:

* Performance benchmarking is essential before production deployment
* GuideLLM provides LLM-specific metrics (TTFT, tokens/sec) beyond generic load testing
* Measured data drives SLA commitments and infrastructure budgeting
* Optimization opportunities identified through utilization metrics

**Business value delivered**:

* **Data-driven SLAs**: Commit to response times based on measured performance
* **Accurate capacity planning**: Calculate exact infrastructure for target load
* **Cost optimization**: Right-size infrastructure based on utilization data
* **Production confidence**: Validate performance before customer-facing launch

**Demonstration complete**:

ACME Corporation has successfully evaluated vLLM Playground / Red Hat AI Inference Server across all critical dimensions:

* **Module 1**: Rapid model deployment via containers (99% faster than manual)
* **Module 2**: Structured outputs for zero-integration-failure backend integration
* **Module 3**: Tool calling for 60-80% reduction in agent handoffs
* **Module 4**: MCP integration for production-ready agentic workflows
* **Module 5**: Performance validation for confident production deployment

ACME is now ready to deploy production AI-powered customer support with quantified business value and technical validation.
