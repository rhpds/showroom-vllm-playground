= Module 5: Performance testing
:source-highlighter: rouge
:toc: macro
:toclevels: 1

In the previous modules, the presenter deployed vLLM servers, configured advanced inferencing features, and built agentic workflows. Now ACME Corporation faces the final question before production deployment: *Can this infrastructure handle real-world load?*

Before launching their AI-powered customer support system, ACME needs to validate that their vLLM deployment can handle expected traffic volumes with acceptable latency. GuideLLM is a performance benchmarking tool designed specifically for LLM inference servers, providing insights into throughput, latency, and resource utilization.

In this module, the presenter benchmarks the vLLM server and demonstrates how to optimize configuration for production workloads.

[[part-1]]
== Part 1 - Performance metrics

=== Know

_Customer challenge: ACME is preparing to launch their AI-powered customer support system, but they have no way to validate whether the infrastructure can handle real-world traffic. Without performance data, they cannot commit to SLAs or plan capacity._

**Business Impact:**

* Production deployments without benchmarking risk outages during peak traffic
* Unknown throughput limits make capacity planning impossible, leading to over-provisioning or customer-facing failures
* Latency targets are guesses rather than validated metrics, undermining SLA commitments
* Infrastructure teams cannot justify GPU spend without concrete performance data

**Value Proposition:**

GuideLLM provides purpose-built benchmarking for LLM inference servers, measuring throughput, latency percentiles, and token generation rates. Combined with vLLM Playground's integrated benchmarking panel, ACME can validate production readiness in minutes and make data-driven infrastructure decisions.

**Key performance metrics and their business impact:**

[cols="1,2,2"]
|===
|Metric |Description |Business Impact

|**Throughput**
|Requests processed per second
|How many customers can be served simultaneously

|**Latency (TTFT)**
|Time to First Token, how quickly the response starts
|User-perceived responsiveness

|**Latency (E2E)**
|End-to-End, total time for complete response
|Total customer wait time

|**Tokens/second**
|Rate of token generation
|How fast responses stream to users

|**GPU Utilization**
|Percentage of GPU compute used
|Infrastructure efficiency and cost

|**Memory Usage**
|GPU memory consumption
|Capacity for concurrent requests
|===

=== Show

**What I say:**

"Before ACME can commit to production SLAs, we need hard numbers. Let me show you how to benchmark the vLLM server with GuideLLM and interpret the results."

**What I do:**

. Connect to the lab environment:
+
[source,bash,subs="attributes"]
----
{ssh_command}
----

. Install GuideLLM:
+
[source,bash]
----
sudo pip install guidellm
----
+
Alternatively, if vLLM Playground was installed with benchmarking support:
+
[source,bash]
----
sudo pip install vllm-playground[benchmark]
----

. Verify the installation:
+
[source,bash]
----
guidellm --help
----
+
Expected output:
+
----
Usage: guidellm [OPTIONS] COMMAND [ARGS]...

  GuideLLM CLI for benchmarking, preprocessing, and testing language models.

Options:
  --version  Show the version and exit.
  --help     Show this message and exit.

Commands:
  benchmark    Run a benchmark or load a previously saved benchmark report.
  config       Show configuration settings.
  mock-server  Start a mock OpenAI/vLLM-compatible server for testing.
  preprocess   Tools for preprocessing datasets for use in benchmarks.
----

. Restart the vLLM Playground daemon service so that GuideLLM can be detected:
+
[source,bash]
----
sudo systemctl restart vllm-playground
----
+
NOTE: The vLLM Playground service needs to be restarted after installing GuideLLM to detect the new benchmarking tools.

. Verify vLLM Playground is running:
+
[source,bash]
----
vllm-playground status
----
+
Or, checking the daemon service:
+
[source,bash]
----
sudo systemctl status vllm-playground
----

. Start a vLLM server with the Qwen model. In the vLLM Playground web UI, configure the following settings:
+
[cols="1,2"]
|===
|Setting |Value

|Model
|`Qwen/Qwen2.5-3B-Instruct`

|Run Mode
|Container

|Compute Mode
|GPU
|===
+
Click *Start Server* and wait for the server to be ready. Monitor the *Server Logs* panel until you see the server is ready to accept connections.
+
NOTE: For performance benchmarking, a simple configuration without tool calling or MCP is used to get accurate baseline metrics.

. Check the vLLM server endpoint:
+
[source,bash]
----
curl http://localhost:8000/v1/models
----
+
This confirms the OpenAI-compatible API is accessible. Note the model name returned, as it is needed for benchmarking.
+
image::module-05-figure-01.png[vLLM models API response,align="center",width=600,link=module-05-figure-01.png^,title="vLLM Models Endpoint"]

. Walk through the GuideLLM benchmark options:
+
[cols="1,2"]
|===
|Option |Purpose

|`--target`
|vLLM server URL (default: http://localhost:8000)

|`--model`
|Model to benchmark (from /v1/models)

|`--rate`
|Request rate (requests/sec) or "sweep"

|`--max-seconds`
|Maximum benchmark duration

|`--max-requests`
|Maximum number of requests

|`--data`
|Dataset: "emulated" or path to custom data

|`--output-path`
|Path to save results
|===

. In the vLLM Playground web UI, navigate to the *GuideLLM* panel in the sidebar.
+
image::module-05-figure-02.png[GuideLLM panel in vLLM Playground,align="center",width=700,link=module-05-figure-02.png^,title="GuideLLM Panel"]

. Select the *GuideLLM (Advanced)* radio button for Benchmark method.
+
NOTE: The *GuideLLM (Advanced)* option is only available after GuideLLM is installed. Without it, the built-in benchmark can still be used for basic performance testing.

. Configure the benchmark settings (defaults):
+
[cols="1,2"]
|===
|Setting |Value

|Total Requests
|`100`

|Request Rate (req/s)
|`5` (requests per second)

|Prompt Tokens
|`100`

|Output Tokens
|`100`

|===

. Click *Run Benchmark* to start the benchmark.
+
This runs 100 requests at 5 request/second using 100 prompt tokens and 100 output tokens.
+
image::module-05-figure-03.png[GuideLLM benchmark in progress,align="center",width=700,link=module-05-figure-03.png^,title="Benchmark Running"]

. Wait for the benchmark to complete. Progress indicators appear, followed by results in the UI.

. Review the benchmark results displayed in the panel. The GuideLLM panel provides three output formats:
+
[cols="1,2"]
|===
|Output Format |Description

|*Raw Output*
|The complete console output from GuideLLM, showing real-time progress and detailed logs

|*JSON*
|Structured JSON output for programmatic analysis and integration with other tools

|*Benchmark Summary Table*
|A formatted table displaying key performance metrics at a glance
|===
+
image::module-05-figure-04.png[GuideLLM benchmark results,align="center",width=700,link=module-05-figure-04.png^,title="Benchmark Results"]
+
The *Benchmark Summary Table* displays four key sections:
+
*Performance Metrics*::
Shows throughput statistics including Mean, Median, Min, and Max requests per second. For example:
+
[cols="1,1,1,1,1"]
|===
|Metric |Mean |Median |Min |Max

|Requests/Second
|4.33
|4.84
|0.00
|59.20
|===

*Token Statistics*::
Displays the token generation rate:
+
[cols="1,1"]
|===
|Metric |Value

|Output Tokens/Second (Mean)
|448.40
|===

*Request Latency Percentiles*::
Shows latency distribution across different percentiles:
+
[cols="1,1,1"]
|===
|Percentile |Latency (s) |Latency (ms)

|P50
|3.417
|3416.56

|P75
|3.498
|3497.91

|P90
|3.512
|3511.61

|P95
|3.518
|3517.99

|P99
|3.559
|3558.93
|===

. Walk through each metric:
+
[cols="1,3"]
|===
|Metric |Interpretation

|**Requests/Second (Mean)**
|Average throughput, at 4.33 req/s this server can handle ~260 requests per minute

|**Output Tokens/Second**
|448 tokens/s indicates the generation speed for responses

|**P50 Latency**
|Median latency, 50% of requests complete within 3.4 seconds

|**P90 Latency**
|90% of requests complete within 3.5 seconds

|**P99 Latency**
|99% of requests complete within 3.6 seconds (tail latency)
|===

. Compare benchmark results against ACME's customer support system requirements:
+
[cols="1,1,2,2"]
|===
|Requirement |Target |Technical Reason |Business Impact

|TTFT (Time to First Token)
|< 500ms
|Users perceive responses as instant below 500ms threshold
|Improves customer satisfaction scores by 25%, reduces abandonment rate from 15% to 5%

|E2E (End-to-End)
|< 3s
|Typical support questions generate 50-100 token responses
|Enables support agents to handle 30 tickets/hour vs 20 tickets/hour (50% productivity gain)

|Throughput
|> 10 req/s
|Peak load during business hours reaches 8-10 concurrent requests
|Supports Black Friday traffic (5x normal load), prevents customer wait times during peak periods, enables 24/7 operation without infrastructure scaling
|===

**What they should notice:**

* GuideLLM installs quickly and integrates directly into the vLLM Playground UI
* Benchmark results provide concrete, quantifiable metrics for production planning
* Latency percentiles (P50, P90, P99) reveal tail latency behavior, not just averages
* The summary table makes it easy to compare results against SLA targets

**If asked:**

Q: "How does GuideLLM compare to other benchmarking tools?"
A: "GuideLLM is purpose-built for LLM inference. It understands token-level metrics like TTFT and output tokens per second, which generic HTTP benchmarking tools like wrk or ab cannot measure."

Q: "Can we run benchmarks without installing GuideLLM?"
A: "Yes, the built-in benchmark in vLLM Playground provides basic performance testing. GuideLLM adds advanced features like sweep testing and detailed latency percentile analysis."

Q: "Are these numbers representative of production performance?"
A: "These are baseline numbers under controlled conditions. Production performance depends on model size, concurrent users, prompt complexity, and hardware. Use these as a starting point and run benchmarks under realistic conditions."

[[part-2]]
== Part 2 - Optimization (optional)

=== Know

_Customer challenge: ACME's baseline benchmarks show the system works, but they need to squeeze more performance out of the same hardware to meet peak traffic demands without additional GPU spend._

**Business Impact:**

* Under-optimized configurations waste GPU resources, increasing infrastructure costs
* Without tuning, peak traffic events (Black Friday, product launches) may overwhelm the system
* One-size-fits-all defaults do not account for ACME's specific workload patterns
* Lack of optimization knowledge means ACME cannot self-service performance improvements

**Value Proposition:**

vLLM Playground exposes key optimization parameters through the UI, enabling teams to experiment with configurations and measure impact through comparative benchmarks. This self-service approach reduces dependency on infrastructure specialists and enables continuous performance improvement.

=== Show

**What I say:**

"Now that we have a baseline, let me walk through the optimization levers available and how ACME can tune their configuration for their specific use case."

**What I do:**

. Review the key optimization parameters available in the vLLM Playground UI:
+
[cols="1,1,2,2"]
|===
|UI Parameter |vLLM Flag |Effect |Trade-off

|*GPU Memory Utilization*
|`--gpu-memory-utilization`
|Higher values (0.9-0.95) allow more concurrent requests
|Too high may cause out-of-memory errors

|*Max Model Length*
|`--max-model-len`
|Maximum context length for requests
|Lower values free memory for more batching

|*Tensor Parallel Size*
|`--tensor-parallel-size`
|Distribute model across multiple GPUs
|Requires multiple GPUs available
|===

. Walk through the optimization workflow:
+
.. *Record the baseline* - Note the key metrics from Part 1 (Requests/Second, Output Tokens/Second, Latency percentiles)
.. *Stop the current server* - Click *Stop Server* in the vLLM Playground UI
.. *Adjust configuration* - Try changing one parameter at a time:
** Increase *GPU Memory Utilization* from 0.9 to 0.95
** Or decrease *Max Model Length* if the use case allows shorter contexts
.. *Restart the server* - Click *Start Server* with the new configuration
.. *Re-run the benchmark* - Use the same GuideLLM settings as Part 1
.. *Compare results* - Did throughput improve? Did latency change?

. Review optimization strategies by use case:
+
[cols="1,2"]
|===
|Use Case |Recommended Approach

|*High throughput* (many concurrent users)
|Increase GPU memory utilization, accept slightly higher latency

|*Low latency* (real-time chat)
|Keep moderate memory utilization (0.85), prioritize fast response times

|*Long contexts* (document analysis)
|Higher max-model-len, fewer concurrent requests
|===

. Invite the audience to try on their own:
+
.. Stop the vLLM server
.. Change one configuration parameter (e.g., GPU Memory Utilization to 0.95)
.. Restart the server and run another benchmark
.. Compare the results with the baseline
+
TIP: Keep notes on what changes were made and how they affected performance. This helps in understanding the trade-offs for specific use cases.

**What they should notice:**

* Optimization is a single-variable experiment: change one parameter, re-benchmark, compare
* The vLLM Playground UI makes it easy to adjust parameters without editing config files
* Small configuration changes can have measurable impact on throughput and latency
* There is no universal "best" configuration; it depends on the workload pattern

**If asked:**

Q: "What is the most impactful parameter to tune first?"
A: "GPU Memory Utilization is usually the best starting point. Increasing from 0.9 to 0.95 can improve concurrent request handling significantly. Just watch for OOM errors."

Q: "Can we automate this optimization process?"
A: "Yes. GuideLLM's sweep mode can automatically test across different request rates. For full configuration sweeps, you can script multiple server restarts with different parameters and compare the results programmatically using the JSON output."

Q: "What about model quantization?"
A: "INT8 and INT4 quantization can dramatically reduce memory usage and improve throughput at the cost of some accuracy. For customer support use cases, quantized models often perform comparably to full-precision models."

== Troubleshooting

**Issue**: "guidellm: command not found"

**Solution**:

. Ensure `sudo pip install` completed successfully
. Check if installed in a virtual environment
. Try: `python3 -m guidellm --help`

**Issue**: "Connection refused" on /v1/models

**Solution**:

. Verify vLLM server is running: `vllm-playground status`
. Check the correct port (default: 8000 for API, 7860 for UI)
. Review server logs: `sudo podman logs vllm-service`

**Issue**: Benchmark requests failing

**Solution**:

. Check server logs: `sudo podman logs vllm-service`
. Verify model is fully loaded
. Reduce request rate and retry

**Issue**: Very slow throughput

**Solution**:

. Verify GPU is being utilized: `nvidia-smi`
. Check model fits in GPU memory
. Consider a smaller model for testing

**Issue**: Out of memory errors

**Solution**:

. Reduce concurrent requests
. Lower `--gpu-memory-utilization` setting
. Use a smaller model

**Issue**: GuideLLM crashes during benchmark

**Solution**:

. Check available system memory
. Reduce `--max-requests` or `--rate`
. Update to latest GuideLLM version

**Issue**: Results vary significantly between runs

**Solution**:

. GPU thermal throttling, allow cooling between benchmarks
. Other processes competing for resources
. Run longer benchmarks for statistical stability

**Issue**: Cannot achieve target performance

**Solution**:

. Current model may be too large for hardware
. Consider model quantization (INT8, INT4)
. Evaluate smaller, faster models for the use case
. Scale horizontally with multiple instances

== Module summary

**What was demonstrated:**

* Installed and configured GuideLLM benchmarking tool
* Ran baseline benchmarks against the vLLM server
* Analyzed throughput, latency (TTFT, E2E), and token generation metrics
* Explored optimization parameters and measured their impact
* Validated results against ACME's production requirements

**Key takeaways:**

* Performance testing is essential before production deployment
* TTFT (Time to First Token) is critical for user-perceived responsiveness
* Throughput vs latency is a fundamental trade-off in LLM serving
* GPU memory utilization directly impacts concurrent request capacity
* Regular benchmarking helps catch performance regressions

**Business impact for ACME:**

* Validated AI infrastructure can handle expected customer load
* Identified optimal configuration for customer support use case
* Established baseline metrics for ongoing monitoring
* Confident production deployment with known performance characteristics
