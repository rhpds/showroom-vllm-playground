= Demo details and presenter guidance
:toc:
:toc-placement: preamble
:icons: font

== Timing and schedule

=== Full demonstration (60 minutes)

* **Module 1**: Getting Started with vLLM Playground (12 minutes)
* **Module 2**: Advanced Inferencing: Structured Outputs (12 minutes)
* **Module 3**: Advanced Inferencing: Tool Calling (12 minutes)
* **Module 4**: Advanced Inferencing: MCP Integration (12 minutes)
* **Module 5**: Performance Testing (12 minutes)

=== Standard demo (30-45 minutes)

* **Module 1**: Getting Started with vLLM Playground (15 minutes)
* **Module 2**: Advanced Inferencing: Structured Outputs (15 minutes)
* **Module 3**: Advanced Inferencing: Tool Calling (15 minutes)

=== Executive brief (15-20 minutes)

* **Module 1**: Getting Started with vLLM Playground (15-20 minutes)

== Technical requirements

=== Software versions

* vLLM Playground v0.1.1
* vLLM v0.10.0
* Podman 4.0 or later
* Python 3.10 or later (required for MCP)
* NVIDIA GPU with CUDA support
* Web browser (Chrome, Firefox, Safari, Edge)

=== Environment access

The demo environment provides:

* GPU-enabled lab host: {targethost}
* SSH access: `{ssh_command}`
* vLLM Playground web UI: link:{vllm_playground_url}[Open vLLM Playground^]
* Pre-installed vLLM Playground CLI

=== Network requirements

* Internet connectivity for downloading container images from registry
* Access to HuggingFace for model downloads (if using gated models)
* Lab host network access (specific URLs provided in lab interface)

== Environment setup

=== Pre-configured environment

The demo environment comes with:

[cols="1,2"]
|===
|Component |Status

|vLLM Playground v0.1.1
|Pre-installed via pip

|Podman
|Pre-installed and configured

|NVIDIA GPU drivers
|Pre-configured with CUDA

|Python 3.10+
|Available for MCP support

|vLLM Container images
|Pre-pulled and verified
|===

=== Optional packages

The following packages can be installed for additional functionality:

[cols="1,2,2"]
|===
|Package |Install Command |Purpose

|MCP Client
|`pip install mcp` or `pip install vllm-playground[mcp]`
|Required for Module 4: MCP Integration

|GuideLLM
|`pip install guidellm` or `pip install vllm-playground[benchmark]`
|Required for Module 5: Performance Testing
|===

NOTE: These packages may already be pre-installed in the demo environment. The modules guide through verification and installation if needed.

=== Setup validation

Run these commands to verify the environment:

[source,bash]
----
# Verify vLLM Playground installation
vllm-playground --help

# Verify Podman
podman version

# Verify GPU availability
nvidia-smi

# Check Python version (for MCP)
python3 --version

# Verify MCP installation (optional - for Module 4)
python3 -c "import mcp; print('MCP installed successfully')"

# Verify GuideLLM installation (optional - for Module 5)
guidellm --help
----

== Troubleshooting guide

=== Common setup issues

**Problem**: "vllm-playground: command not found"
-> **Solution**: Verify the installation path is in your PATH, or run with full path

**Problem**: "Permission denied" when running Podman commands
-> **Solution**: Ensure you are using rootless Podman or have appropriate permissions

**Problem**: "NVIDIA driver not found" or GPU not detected
-> **Solution**: Verify NVIDIA drivers are installed with `nvidia-smi`

**Problem**: "Container image pull fails"
-> **Solution**: Check network connectivity and ensure you have access to container registries

**Problem**: "Port 7860 already in use"
-> **Solution**: Run `vllm-playground stop` to stop any existing instance, or use `--port` to specify a different port

=== During demo support

* Check the vLLM Playground logs: `sudo podman logs vllm-service`
* Verify server status: `vllm-playground status`
* Restart if needed: `sudo systemctl restart vllm-playground`

== Follow-up resources

=== Next steps after the demo

* link:https://github.com/micytao/vllm-playground[vLLM Playground GitHub Repository^] - Source code and documentation
* link:https://github.com/vllm-project/vllm[vLLM Project^] - The underlying high-performance inference engine
* link:https://github.com/vllm-project/guidellm[GuideLLM Documentation^] - Performance benchmarking tool
* link:https://modelcontextprotocol.io[Model Context Protocol^] - MCP specification and servers

=== Additional learning paths

* **Intermediate**: Explore different model architectures and their tool calling capabilities
* **Advanced**: Deploy vLLM Playground on OpenShift/Kubernetes for enterprise scale
* **Production**: Implement custom MCP servers for specific use cases

== Authors and contributors

**Primary Author**: Michael Yang
**Last Updated**: January 2026
**Demo Version**: 1.0

**Feedback and Questions**:

* Demo issues: Submit via the lab feedback mechanism
* vLLM Playground issues: link:https://github.com/micytao/vllm-playground/issues[GitHub Issues^]
