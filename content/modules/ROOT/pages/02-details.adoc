= Demonstration details and presenter guidance
:toc:
:toc-placement: preamble
:icons: font

== Demo timing and flexible formats

=== Full demonstration (60 minutes)

* **Module 1**: Getting Started with vLLM Playground (12 minutes)
* **Module 2**: Structured Outputs for System Integration (12 minutes)
* **Module 3**: Tool Calling for Dynamic Functionality (12 minutes)
* **Module 4**: MCP Integration for Agentic Workflows (12 minutes)
* **Module 5**: Performance Testing and Validation (12 minutes)

=== Standard technical demo (30-45 minutes)

**Recommended flow**:

* **Module 1**: Getting Started with vLLM Playground (12 minutes)
* **Module 2**: Structured Outputs (10 minutes - focus on JSON Schema only)
* **Module 3**: Tool Calling (10 minutes - show one or two tool examples)
* **Q&A and discussion** (8-13 minutes)

**What to skip**: Module 4 (MCP Integration) and Module 5 (Performance Testing) - mention these capabilities briefly

=== Executive brief (15-20 minutes)

**Recommended flow**:

* **Module 1 Part 1**: Deploy vLLM server and show chat interface (8 minutes)
* **Quick highlights**: Show structured output example and mention tool calling (4 minutes)
* **Business value recap**: Connect capabilities to ACME's challenges (3 minutes)
* **Q&A and next steps** (5 minutes)

**What to skip**: Detailed technical configurations, performance testing, MCP integration

== Technical specifications

=== Software versions used in this demonstration

* vLLM Playground v0.1.1
* vLLM v0.10.0
* Podman 4.0 or later
* Python 3.10 or later (for MCP demonstrations)
* NVIDIA GPU with CUDA support
* Web browser (Chrome, Firefox, Safari, Edge)

=== Demo environment specifications

Presenters demonstrate on a pre-configured environment with:

* GPU-enabled demonstration host: {targethost}
* SSH access available: `{ssh_command}`
* vLLM Playground web UI: link:http://{hostname}:7860[Open vLLM Playground^]
* Pre-installed vLLM Playground CLI
* Pre-pulled container images for faster demonstrations

=== Network requirements for live demos

* Internet connectivity for accessing web UI (audience view)
* Access to container registries (if demonstrating image pulls)
* Lab host network access for presenter SSH connections

NOTE: For customer site demonstrations, verify firewall rules allow access to demo host port 7860 (web UI) and SSH port 22.

== Presenter preparation and environment setup

=== Pre-demo environment validation

Before starting your demonstration, verify the environment is ready:

[source,bash]
----
# Verify vLLM Playground installation
vllm-playground --help

# Check Podman availability
podman version

# Verify GPU (optional - depends on demo depth)
nvidia-smi

# Check system daemon status (if configured)
sudo systemctl status vllm-playground
----

=== Demo environment components

Your demonstration environment includes:

[cols="1,2"]
|===
|Component |Status

|vLLM Playground v0.1.1
|✓ Pre-installed via pip

|Podman
|✓ Pre-installed and configured

|NVIDIA GPU drivers
|✓ Pre-configured with CUDA (L4 or equivalent)

|Python 3.10+
|✓ Available for MCP demonstrations

|vLLM Container images
|✓ Pre-pulled for faster demo flow
|===

=== Optional packages for extended demos

For advanced demonstration modules, these packages are available:

[cols="1,2,2"]
|===
|Package |Purpose |Demo Module

|MCP Client
|Model Context Protocol integration
|Module 4: MCP Integration

|GuideLLM
|Performance benchmarking and metrics
|Module 5: Performance Testing
|===

== Presenter support and troubleshooting

=== Common demo scenarios and handling

**Scenario**: Web UI doesn't load during demonstration
→ **Recovery**: Use SSH terminal to show CLI commands and explain web UI features. Most audiences understand technical glitches.

**Scenario**: Model inference is slower than expected
→ **Context**: Explain that cold starts take longer, warm inference is faster. Use it as opportunity to discuss production optimization.

**Scenario**: Audience asks about specific model support
→ **Reference**: vLLM supports Llama, Mistral, Qwen, Granite, and many others. Check link:https://github.com/vllm-project/vllm[vLLM supported models list^].

**Scenario**: Question about enterprise deployment and support
→ **Answer**: Red Hat AI Inference Server (RHAIIS) provides enterprise-supported version of vLLM with SLAs, security updates, and OpenShift integration.

=== Live demonstration support commands

Keep these handy for demonstration recovery:

[source,bash]
----
# Check vLLM Playground status
vllm-playground status

# View server logs
podman logs vllm-service

# Restart if needed (last resort)
vllm-playground stop && vllm-playground

# Check running containers
podman ps
----

== Audience engagement and follow-up resources

=== Recommended post-demo actions for attendees

After this demonstration, interested attendees should:

. **Try it yourself**: Access link:https://github.com/micytao/vllm-playground[vLLM Playground GitHub Repository^] for installation and setup
. **Request POC environment**: Contact your Red Hat representative for proof-of-concept environment provisioning
. **Schedule deep dive**: Book technical architecture session to discuss integration with your specific systems

=== Additional learning and technical resources

**vLLM and inference engine resources**:

* link:https://github.com/vllm-project/vllm[vLLM Project^] - The high-performance inference engine powering this demonstration
* link:https://github.com/micytao/vllm-playground[vLLM Playground GitHub Repository^] - Source code, documentation, and community
* link:https://github.com/vllm-project/guidellm[GuideLLM Documentation^] - Performance benchmarking tool demonstrated in Module 5

**Model Context Protocol and agentic AI**:

* link:https://modelcontextprotocol.io[Model Context Protocol Specification^] - MCP standard and server implementations
* link:https://github.com/modelcontextprotocol/servers[MCP Servers Repository^] - Pre-built MCP servers for common integrations

**Red Hat AI and enterprise deployment**:

* link:https://www.redhat.com/en/technologies/cloud-computing/openshift/openshift-ai[Red Hat OpenShift AI^] - Enterprise AI platform with vLLM support
* link:https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed[OpenShift AI Documentation^] - Deployment and configuration guides
* link:https://developers.redhat.com/products/openshift-ai[Red Hat Developers: OpenShift AI^] - Developer resources and tutorials

=== Sales enablement and next steps

**For sales engineers and solution architects**:

After delivering this demonstration, follow up with prospects by:

. **Sending demonstration recording**: Email link with timestamps for key moments
. **Sharing technical resources**: Provide links to documentation and trial access
. **Scheduling POC planning**: Book session to scope proof-of-concept requirements
. **Connecting with product specialists**: Loop in AI/ML specialists for deeper technical discussions

**Qualification questions to assess fit**:

* What's your current AI inference infrastructure? (understand baseline)
* How many models do you need to serve? (assess scale requirements)
* What systems need AI integration? (identify tool calling opportunities)
* When do you need production deployment? (timeline and urgency)
* What's your biggest AI infrastructure challenge? (align solution to pain)

== Authors and contributors

**Primary Author**: Michael Yang (vLLM Playground) +
**Demo Content**: Red Hat Showroom Team +
**Last Updated**: January 2026 +
**Demo Version**: 1.0

**Feedback and questions**:

* Demo delivery feedback: Submit via Red Hat Showroom feedback mechanism
* vLLM Playground technical issues: link:https://github.com/micytao/vllm-playground/issues[GitHub Issues^]
* Red Hat AI solutions: Contact your Red Hat account team
